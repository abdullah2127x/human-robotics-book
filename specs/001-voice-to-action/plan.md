# Chapter 10: Voice-to-Action - Using OpenAI Whisper for Voice Commands — Lesson Plan

**Generated by**: chapter-planner v2.0.0 (Reasoning-Activated)
**Source Spec**: specs/001-voice-to-action/spec.md
**Created**: 2025-12-17
**Constitution**: v6.0.1 (Reasoning Mode)

---

## I. Chapter Analysis

### Chapter Type
**Technical/Code-Focused** — Voice command pipeline implementation for humanoid robots. Chapter teaches implementation skills (Whisper setup, audio capture, intent parsing, ROS 2 integration). Learning objectives emphasize "configure," "implement," "design," "measure" (action verbs indicating hands-on technical work). Code examples (Python audio capture, Whisper API calls, intent parsers, ROS 2 nodes) and practical exercises required throughout.

### Concept Density Analysis

**Core Concepts** (from Part 4 spec): 12 major concepts
1. Speech recognition fundamentals (acoustic models, language models)
2. OpenAI Whisper architecture (encoder-decoder, multilingual)
3. Whisper API vs local model tradeoffs
4. Audio streaming (real-time capture, buffering, VAD)
5. Voice Activity Detection (VAD) for command segmentation
6. Wake word detection (optional but common pattern) — OUT OF SCOPE
7. Intent parsing (command structure, entities, parameters)
8. Natural Language Understanding (NLU) basics
9. ROS 2 integration (audio → intent → action topic/service)
10. Error handling (unclear speech, background noise, timeout)
11. Latency optimization (streaming vs batch processing)
12. Privacy and safety considerations (voice data handling)

**Complexity Assessment**: **Complex** — 12 concepts (11 in scope), high complexity. Voice interfaces require understanding multiple domains: audio processing, machine learning (Whisper), NLU (intent parsing), and robotics integration (ROS 2). Prerequisites from Parts 1-3 provide foundation, but voice processing is new paradigm.

**Proficiency Tier**: C1 (Advanced Integration) from chapter-index.md
- C1: Can configure voice pipeline, design intent schemas, debug failures independently
- Light scaffolding (student-driven exploration)

**Justified Lesson Count**: 9 lessons
- Layer 1 (Manual): 2 lessons (speech recognition fundamentals, Whisper setup)
- Layer 2 (AI Collaboration): 4 lessons (audio capture, VAD, intent parsing, ROS 2 integration)
- Layer 3 (Intelligence Design): 2 lessons (voice-intent-parsing skill, audio-pipeline-optimization skill)
- Layer 4 (Capstone): 1 lesson (full voice pipeline integrated with humanoid)
- **Total**: 9 lessons

**Reasoning**: High concept density (11 concepts) at C1 proficiency justifies 9 lessons. Complex multi-domain topic (audio + ML + NLU + ROS 2) requires solid Layer 1 foundation (2 lessons). AI collaboration essential for intent schema design (4 lessons). Two Layer 3 lessons encode reusable patterns. Capstone integrates voice pipeline with humanoid from Parts 2-3.

---

## II. Success Criteria (from Spec)

### Technical Success Evals
- **Eval-10.1**: Students configure and run Whisper for speech-to-text
- **Eval-10.2**: Students implement real-time audio capture with PyAudio/sounddevice
- **Eval-10.3**: Students detect voice activity and segment commands
- **Eval-10.4**: Students parse voice commands into structured intents
- **Eval-10.5**: Students publish intents to ROS 2 topics
- **Eval-10.6**: Students handle voice recognition errors gracefully
- **Eval-10.7**: Students optimize for acceptable latency (&lt;2 seconds end-to-end)

### Pedagogical Success
- [ ] All lessons follow 4-layer teaching framework
- [ ] Specification-first pattern (design intent schemas before implementing parsers)
- [ ] Iterative refinement pattern (test → fail → debug → improve cycle)
- [ ] "Try With AI" sections demonstrate AI collaboration (no meta-commentary)
- [ ] Exercises have checkbox success criteria
- [ ] Capstone integrates Part 4 concepts with Parts 1-3 foundation

---

## III. Lesson Sequence

### Lesson 1: Speech Recognition Fundamentals (Layer 1: Manual Foundation)

**Learning Objective**: Understand speech recognition pipeline stages (audio → features → model → text) by manually examining audio waveforms, spectrograms, and transcription outputs.

**Stage**: 1 (Manual Foundation)

**CEFR Proficiency**: C1

**New Concepts** (count: 7 — within C1 limit of 10-12):
1. Speech recognition problem definition (acoustic signal → text)
2. Audio representation (waveforms, samples, sample rate)
3. Feature extraction (spectrograms, Mel-frequency cepstral coefficients)
4. Acoustic models (mapping features to phonemes)
5. Language models (predicting word sequences)
6. End-to-end models (encoder-decoder architecture)
7. Evaluation metrics (Word Error Rate, accuracy)

**Cognitive Load Validation**: 7 concepts ≤ 12 limit (C1) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-10.1 (foundational for Whisper understanding)

**Content Elements**:
- **Speech recognition motivation**: Why voice interfaces for robots? (natural human communication)
- **Manual audio exploration**: Students examine audio waveforms in Audacity or Python (visualize what Whisper "sees")
- **Spectrogram analysis**: Students generate spectrograms from voice recordings, understand time-frequency representation
- **No AI assistance yet**: Build mental model of speech → features → text pipeline
- **Practice**: Students record voice commands, visualize waveforms and spectrograms, understand audio properties
- **Checkpoint**: Students explain in own words: "How does audio become features? How do features become text?"

**Prerequisites**: Part 1 (Python basics), Part 2 (sensor data concepts)

**Estimated Time**: 90 minutes

**Teaching Pattern**: Specification-first (define speech recognition quality criteria before using Whisper)

---

### Lesson 2: Setting Up OpenAI Whisper (Layer 1: Manual Foundation)

**Learning Objective**: Install and configure OpenAI Whisper (API and local options), understand model selection tradeoffs, and verify transcription quality by manual testing.

**Stage**: 1 (Manual Foundation)

**CEFR Proficiency**: C1

**New Concepts** (count: 8 — within C1 limit):
1. OpenAI Whisper architecture (encoder-decoder transformer)
2. Whisper model sizes (tiny, base, small, medium, large)
3. Model selection tradeoffs (accuracy vs latency vs compute)
4. Whisper API (cloud-based, pay-per-use)
5. Local Whisper installation (whisper.cpp, faster-whisper)
6. Transcription output format (text, timestamps, confidence)
7. Language detection and forced language mode
8. Batch vs streaming transcription modes

**Cognitive Load Validation**: 8 concepts ≤ 12 limit (C1) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-10.1 (configure and run Whisper)

**Content Elements**:
- **API setup**: Students create OpenAI API key, test Whisper API with curl/Python
- **Local setup**: Students install faster-whisper or whisper.cpp, compare to API
- **Model comparison**: Students run same audio through different model sizes, measure accuracy vs speed
- **Configuration exploration**: Students manually adjust parameters (language, temperature, prompt)
- **No AI collaboration yet**: Build understanding of Whisper options manually
- **Practice**: Students transcribe 10 test commands (navigation, manipulation), measure Word Error Rate
- **Checkpoint**: Whisper installed, transcription working, model tradeoffs understood

**Prerequisites**: Lesson 1 (speech recognition theory)

**Estimated Time**: 120 minutes

**Teaching Pattern**: Hands-on discovery (experiment with Whisper configurations)

---

### Lesson 3: Real-Time Audio Capture (Layer 2: AI Collaboration)

**Learning Objective**: Implement real-time audio capture from microphone using PyAudio/sounddevice, with proper buffering and sample rate configuration, using AI collaboration for debugging audio issues.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: C1

**New Concepts** (count: 7 — within C1 limit):
1. Audio input devices (microphone selection, USB audio)
2. PyAudio vs sounddevice (library comparison)
3. Sample rate configuration (16kHz for Whisper)
4. Audio buffering (chunk size, callback vs blocking)
5. Real-time constraints (processing faster than audio arrival)
6. Audio format conversion (bytes → numpy array → Whisper input)
7. Microphone troubleshooting (device enumeration, permissions)

**Cognitive Load Validation**: 7 concepts ≤ 12 limit (C1) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-10.2 (implement real-time audio capture)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student's audio capture has crackling/popping artifacts
   - *AI teaches*: "Audio artifacts usually indicate buffer underrun—processing is slower than audio arrival. Solution: Increase buffer size or use callback-based capture instead of blocking. Here's the tradeoff: [explains latency vs stability]"
   - *Learning*: Student learns buffer management for real-time audio

2. **AI as Student**:
   - *Scenario*: AI suggests 44.1kHz sample rate (CD quality, excessive for speech)
   - *Student teaches*: "Whisper expects 16kHz audio. Higher sample rate wastes bandwidth and requires resampling. Here's the constraint: [specifies 16kHz requirement]"
   - *Adaptation*: AI configures capture at correct sample rate

3. **AI as Co-Worker**:
   - *Scenario*: Student and AI iterate on buffer size selection
   - *Iteration 1*: Student uses small buffer (512 samples) → audio drops
   - *Iteration 2*: AI suggests large buffer (8192 samples) → high latency
   - *Iteration 3*: Together identify optimal buffer (1024-2048) → stable + responsive
   - *Convergence*: Collaboration found buffer size that balances reliability and latency

**Content Elements**:
- **Recap Layer 1**: Review audio fundamentals from Lesson 1
- **Manual audio capture**: Student implements basic capture (will likely have issues)
- **AI collaboration**: Debug audio capture problems (show all three roles)
- **Practice exercise**: Students capture audio, verify waveform in real-time, feed to Whisper
- **Validation checkpoint**: Audio captures without drops, correct sample rate, Whisper transcribes live audio

**Prerequisites**: Lessons 1-2 (speech recognition theory, Whisper setup)

**Estimated Time**: 120 minutes

---

### Lesson 4: Voice Activity Detection (Layer 2: AI Collaboration)

**Learning Objective**: Implement Voice Activity Detection (VAD) to segment speech from silence, enabling command boundary detection, using AI collaboration for threshold tuning.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: C1

**New Concepts** (count: 7 — within C1 limit):
1. Voice Activity Detection problem (speech vs silence vs noise)
2. Energy-based VAD (amplitude thresholding)
3. Spectral VAD (frequency-based speech detection)
4. WebRTC VAD (Google's production VAD library)
5. Silero VAD (neural network-based VAD)
6. Command segmentation (speech onset → speech offset → complete command)
7. VAD tuning (threshold, hangover time, minimum speech duration)

**Cognitive Load Validation**: 7 concepts ≤ 12 limit (C1) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-10.3 (detect voice activity and segment commands)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student's VAD triggers on background noise (false positives)
   - *AI teaches*: "Energy-based VAD is noise-sensitive. For noisy environments, use neural VAD (Silero) or WebRTC VAD with higher aggressiveness. Here's the tradeoff: [explains sensitivity vs robustness]"
   - *Learning*: Student learns VAD algorithm selection for environment

2. **AI as Student**:
   - *Scenario*: AI suggests very short hangover time (cuts off speech mid-word)
   - *Student teaches*: "Natural speech has pauses (breaths, thinking). Hangover time needs to be 0.5-1.0 seconds to avoid cutting commands. Here's the constraint: [specifies minimum hangover]"
   - *Adaptation*: AI configures VAD with appropriate hangover

3. **AI as Co-Worker**:
   - *Scenario*: Student and AI iterate on VAD threshold
   - *Iteration 1*: Student uses low threshold → triggers on breathing
   - *Iteration 2*: AI suggests high threshold → misses soft speech
   - *Iteration 3*: Together identify adaptive threshold → robust across volumes
   - *Convergence*: Collaboration tuned VAD for reliable command segmentation

**Content Elements**:
- **VAD theory**: Why is VAD needed? (continuous listening without transcribing silence)
- **Manual VAD testing**: Student implements energy-based VAD (will have issues with noise)
- **AI collaboration**: Tune VAD parameters, select algorithm (show all three roles)
- **Practice exercise**: Students test VAD in quiet and noisy environments
- **Validation checkpoint**: VAD segments commands correctly, &lt;10% false positive rate

**Prerequisites**: Lesson 3 (audio capture working)

**Estimated Time**: 120 minutes

---

### Lesson 5: Designing Intent Schemas (Layer 2: AI Collaboration)

**Learning Objective**: Design structured intent schemas for robotics commands (navigation, manipulation, queries, control) using specification-first approach with AI collaboration.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: C1

**New Concepts** (count: 8 — within C1 limit):
1. Intent representation (action, target, parameters, confidence)
2. Intent categories for robotics (navigate, pick, place, query, control)
3. Entity extraction (locations, objects, colors, quantities)
4. Slot filling (required vs optional parameters)
5. Ambiguity handling (missing targets, unclear commands)
6. Intent schema design patterns (hierarchical, flat)
7. Natural language variations (synonyms, phrasings)
8. Fallback intents (unknown commands)

**Cognitive Load Validation**: 8 concepts ≤ 12 limit (C1) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-10.4 (parse voice commands into structured intents)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student designs flat intent schema without entity extraction
   - *AI teaches*: "Flat schemas like 'go_to_kitchen' don't generalize. Extract entities: action='navigate', target='kitchen'. This supports any location without new intents. Here's the pattern: [shows entity extraction approach]"
   - *Learning*: Student learns scalable intent schema design

2. **AI as Student**:
   - *Scenario*: AI suggests complex nested schema (over-engineered)
   - *Student teaches*: "Our humanoid only needs 5 action types for MVP. Complex hierarchy adds parsing overhead without benefit. Here's the constraint: [specifies supported actions]"
   - *Adaptation*: AI simplifies schema to match robot capabilities

3. **AI as Co-Worker**:
   - *Scenario*: Student and AI iterate on ambiguity handling
   - *Iteration 1*: Student ignores ambiguous commands → robot does wrong thing
   - *Iteration 2*: AI suggests rejecting all ambiguity → frustrating user experience
   - *Iteration 3*: Together design confidence-based approach → ask clarification only when truly ambiguous
   - *Convergence*: Collaboration designed robust ambiguity handling

**Content Elements**:
- **Specification-first**: Students write intent schema BEFORE implementing parser
- **Schema design exercise**: Define intents for navigation, manipulation, queries, control
- **AI collaboration**: Refine schema for edge cases (show all three roles)
- **Practice exercise**: Students test schema against 20 example commands
- **Validation checkpoint**: Schema covers all command types, handles ambiguity

**Prerequisites**: Lessons 1-4 (audio pipeline complete)

**Estimated Time**: 120 minutes

---

### Lesson 6: Implementing Intent Parsers (Layer 2: AI Collaboration)

**Learning Objective**: Implement intent parsers that convert transcribed text to structured intents using rule-based and LLM-based approaches with AI collaboration.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: C1

**New Concepts** (count: 7 — within C1 limit):
1. Rule-based parsing (regex, keyword matching)
2. LLM-based parsing (prompt engineering for intent extraction)
3. Hybrid approaches (rules + LLM fallback)
4. Entity recognition (spaCy, regex for locations/objects)
5. Confidence scoring (parser certainty)
6. Parser testing (unit tests for intent parsing)
7. Parser performance (latency considerations)

**Cognitive Load Validation**: 7 concepts ≤ 12 limit (C1) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-10.4 (parse voice commands into structured intents)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student's rule-based parser fails on "head to the kitchen" (synonym of "go to")
   - *AI teaches*: "Rule-based parsers are brittle with synonyms. Options: expand rules OR use LLM for intent classification. Here's the tradeoff: [explains rules vs LLM for parsing]"
   - *Learning*: Student learns parser approach selection

2. **AI as Student**:
   - *Scenario*: AI suggests using GPT-4 for all parsing (expensive, slow)
   - *Student teaches*: "LLM API calls add 500ms+ latency. For simple commands, rules are faster. Reserve LLM for ambiguous cases. Here's the performance budget: [specifies latency target]"
   - *Adaptation*: AI designs hybrid parser (rules first, LLM fallback)

3. **AI as Co-Worker**:
   - *Scenario*: Student and AI iterate on entity extraction
   - *Iteration 1*: Student uses simple regex → misses "the red ball on the table"
   - *Iteration 2*: AI suggests spaCy NER → works but slow
   - *Iteration 3*: Together design targeted regex + adjective extraction → fast + accurate
   - *Convergence*: Collaboration created parser that meets latency and accuracy goals

**Content Elements**:
- **Parser implementation**: Students implement parser for schema from Lesson 5
- **Testing framework**: Unit tests for each intent type and edge case
- **AI collaboration**: Debug parsing failures (show all three roles)
- **Practice exercise**: Students parse 50 test commands, measure accuracy
- **Validation checkpoint**: Parser achieves &gt;90% accuracy on test set

**Prerequisites**: Lesson 5 (intent schema defined)

**Estimated Time**: 120 minutes

---

### Lesson 7: ROS 2 Integration (Layer 2: AI Collaboration)

**Learning Objective**: Build ROS 2 node that publishes parsed intents to topics, enabling downstream systems (Nav2, manipulators) to execute voice commands with AI collaboration for message design.

**Stage**: 2 (AI Collaboration — consolidation, less emphasis on Three Roles)

**CEFR Proficiency**: C1

**New Concepts** (count: 7 — within C1 limit):
1. Custom ROS 2 message types (VoiceIntent.msg)
2. Voice node architecture (audio → Whisper → parser → publisher)
3. Topic design (/voice_intent, /voice_diagnostics)
4. Lifecycle node patterns (managed startup/shutdown)
5. Parameter server integration (runtime configuration)
6. Nav2 goal conversion (intent → navigation goal)
7. Latency measurement in ROS 2 (timestamp propagation)

**Cognitive Load Validation**: 7 concepts ≤ 12 limit (C1) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-10.5 (publish intents to ROS 2 topics)

**Content Elements**:
- **Message definition**: Students define VoiceIntent.msg with action, target, parameters, confidence
- **Node implementation**: Students build voice_command_node using rclpy
- **Integration testing**: Verify with `ros2 topic echo /voice_intent`
- **Nav2 bridge**: Convert navigation intents to Nav2 goals (connection to Part 3)
- **Practice exercise**: Students speak commands, verify intents reach subscribers
- **Validation checkpoint**: Full pipeline working: speak → transcribe → parse → publish → receive

**Prerequisites**: Lessons 1-6 (voice pipeline complete), Part 1 (rclpy fundamentals)

**Estimated Time**: 120 minutes

---

### Lesson 8: Creating Voice-Intent-Parsing Skill (Layer 3: Intelligence Design)

**Learning Objective**: Encode voice intent parsing patterns into reusable skill for future voice interface projects.

**Stage**: 3 (Intelligence Design)

**CEFR Proficiency**: C1

**Reusable Artifact Created**: **voice-intent-parsing-skill**

**Maps to Evals**: Eval-10.4 (intent parsing patterns)

**Skill Design Framework** (Persona + Questions + Principles):

**Persona Definition**:
"Think like a voice interface designer creating intent parsers for robotics applications. Your goal is to transform natural language commands into structured, actionable intents that robots can execute reliably."

**Question Structure**:
1. What command categories does the robot support? (navigation, manipulation, queries, control?)
2. What entities need extraction? (locations, objects, colors, quantities?)
3. What's the confidence threshold for execution vs clarification?
4. How do I handle ambiguous or incomplete commands?
5. What latency budget exists for parsing? (rules fast, LLM slow)

**Principle Articulation**:
1. **Schema Before Parser**: Define intent schema before implementing parser (specification-first)
2. **Entity Extraction**: Extract reusable entities (locations, objects) rather than hardcoding (go_to_kitchen)
3. **Confidence-Based Execution**: High confidence → execute, low confidence → clarify
4. **Hybrid Parsing**: Rules for common patterns, LLM for complex/ambiguous
5. **Fallback Gracefully**: Unknown commands → acknowledge + suggest alternatives

**Content Elements**:
- **Review Lessons 5-6**: What parsing patterns did we design?
- **Pattern extraction**: What's reusable across different robots/domains?
- **Skill creation**: Students write skill using Persona + Questions + Principles template
- **Usage validation**: Test skill on different command vocabulary (home automation, manufacturing)
- **Documentation**: Students document intent schema design checklist

**Prerequisites**: Lessons 5-6 (intent parsing experience)

**Estimated Time**: 90 minutes

---

### Lesson 9: Capstone - Voice-Controlled Humanoid Navigation (Layer 4: Spec-Driven Integration)

**Learning Objective**: Implement complete voice command pipeline integrated with humanoid robot using specification-first approach, composing all accumulated skills and connecting to Nav2 from Part 3.

**Stage**: 4 (Spec-Driven Integration / Capstone)

**CEFR Proficiency**: C1

**Maps to Evals**: ALL chapter evals integrated

**Project Specification** (Spec FIRST, then implementation):

**Intent**:
Implement voice-controlled humanoid navigation system. Student speaks navigation commands ("Go to the kitchen"), system transcribes with Whisper, parses to intent, publishes to ROS 2, and humanoid navigates using Nav2 from Part 3.

**Constraints**:
- End-to-end latency: &lt;2 seconds from speech completion to navigation start
- Accuracy: &gt;90% transcription accuracy in quiet environment
- Robustness: Handle errors gracefully (unclear speech, API failures)
- Integration: Connect to Nav2 stack from Part 3 Chapter 9

**Success Criteria**:
- [ ] Voice command "Go to the kitchen" triggers humanoid navigation to kitchen waypoint
- [ ] Voice command "Stop" halts navigation immediately
- [ ] System asks for clarification on ambiguous commands ("Go there")
- [ ] Full pipeline latency &lt;2 seconds measured
- [ ] Error handling works (API failure → graceful degradation)
- [ ] Integration with Isaac Sim or Gazebo humanoid from Parts 2-3

**Component Composition**:
- **Lesson 8 Skills**: Apply voice-intent-parsing-skill
- **Part 3 Chapter 9**: Nav2 navigation stack, behavior trees
- **Part 1 Chapter 2**: rclpy patterns, async/await

**Workflow**:
1. **Specification Writing** (FIRST): Students write detailed spec.md defining voice command requirements
2. **Component Integration**:
   - Audio capture + VAD (Lessons 3-4)
   - Whisper transcription (Lesson 2)
   - Intent parsing (Lessons 5-6)
   - ROS 2 publishing (Lesson 7)
   - Nav2 goal conversion (Part 3 integration)
3. **Iteration Loop**:
   - Test voice commands in simulation
   - Apply voice-intent-parsing skill for parsing refinement
   - Measure latency, optimize bottlenecks
4. **Final Validation**: 5-minute voice-controlled navigation demo

**Content Elements**:
- **Spec-first requirement**: Students write specification BEFORE integration
- **Full chapter integration**: All lessons combined into working system
- **Cross-part integration**: Connect to Nav2 from Part 3
- **AI orchestration**: AI integrates components from spec
- **Success validation**: All criteria met (latency, accuracy, error handling)

**Prerequisites**: All lessons from Chapter 10, Part 3 Chapter 9 (Nav2)

**Estimated Time**: 180 minutes (comprehensive Part 4 Chapter 10 capstone)

---

## IV. Skill Dependencies

**Skill Dependency Graph**:
```
Part 1 rclpy patterns → Lesson 7 (ROS 2 node)
Part 2 sensor concepts → Lesson 1 (audio as sensor)
Part 3 Nav2 → Lesson 9 (capstone navigation integration)

Lesson 1 (speech fundamentals) → Lesson 2 (Whisper setup)
Lesson 2 (Whisper) → Lesson 3 (audio capture)
Lesson 3 (audio capture) → Lesson 4 (VAD)
Lesson 4 (VAD) → Lesson 5 (intent schemas)
Lesson 5 (intent schemas) → Lesson 6 (intent parsers)
Lesson 6 (intent parsers) → Lesson 7 (ROS 2 integration)
Lessons 5-6 (parsing experience) → Lesson 8 (voice-intent-parsing skill)
Lessons 1-8 + Part 3 Nav2 → Lesson 9 (capstone)
```

**Cross-Chapter Dependencies**:
- Part 1 (ROS 2 nodes, topics, rclpy) → Required for Lessons 7, 9
- Part 2 (sensor concepts) → Conceptual foundation for Lesson 1
- Part 3 Chapter 9 (Nav2) → Required for Lesson 9 capstone integration
- Validation: ✅ Prerequisites implemented

---

## V. Assessment Plan

### Formative Assessments (During Lessons)
- Lesson 2: Whisper transcription quality (Word Error Rate measurement)
- Lesson 4: VAD performance (&lt;10% false positive rate)
- Lesson 6: Intent parsing accuracy (&gt;90% on test set)
- Lesson 7: ROS 2 integration (intents received by subscribers)

### Summative Assessment (End of Chapter)
- Lesson 9: Capstone evaluation (voice → navigation working, latency &lt;2s, errors handled)

**All assessments align with C1 proficiency + Bloom's Analyze/Evaluate/Create levels**

---

## VI. Validation Checklist

**Chapter-Level Validation**:
- [x] Chapter type identified (technical/code-focused)
- [x] Concept density analysis documented (11 concepts → 9 lessons justified)
- [x] Lesson count justified by concept density + C1 tier
- [x] All evals from spec covered by lessons
- [x] All lessons map to at least one eval

**Stage Progression Validation**:
- [x] Lessons 1-2: Layer 1 (Manual fundamentals + Whisper setup, no AI)
- [x] Lessons 3-7: Layer 2 (AI Collaboration with Three Roles)
- [x] Lesson 8: Layer 3 (Intelligence Design, reusable skill)
- [x] Lesson 9: Layer 4 (Spec-Driven Integration, capstone)
- [x] Specification-first pattern applied (Lesson 5: schema before parser; Lesson 9: spec before integration)

**Cognitive Load Validation**:
- [x] C1 lessons: 7-8 concepts ≤ 12 limit
- [x] Complex multi-domain concepts distributed across lessons

**Dependency Validation**:
- [x] Skill dependencies satisfied by lesson order
- [x] Cross-chapter dependencies validated (Part 1-3 prerequisites)

**Three Roles Validation** (Layer 2 lessons):
- [x] Lessons 3-6 each demonstrate AI as Teacher
- [x] Lessons 3-6 each demonstrate AI as Student
- [x] Lessons 3-6 each demonstrate AI as Co-Worker (convergence)

**Teaching Modality Validation**:
- [x] Primary: Specification-first (Lesson 5 - design schema before parser; Lesson 9 - spec before integration)
- [x] Secondary: Iterative refinement (Lessons 3-4, 6 - test → debug → improve cycle)
- [x] Distinct from Part 3 Chapter 9 (collaborative parameter tuning + BT design)

---

## VII. Anti-Convergence Notes

**Part 3 Chapter 9 Pattern**: Collaborative parameter tuning + Behavior tree design
**Part 3 Chapter 8 Pattern**: Error analysis + Specification-first

**Part 4 Chapter 10 Variation**:
- **Primary**: Specification-first (Lesson 5 designs intent schema BEFORE implementing parser; Lesson 9 writes spec BEFORE integration)
- **Secondary**: Iterative refinement (Lessons 3-4, 6 use test → fail → debug → improve cycle for audio and parsing)
- **Rationale**: Voice interfaces require upfront design (intent schemas must be defined before parsing can work). Iterative refinement suits real-world voice input messiness.

**Result**: Teaching modality distinct from Part 3 chapters. Chapter 10 uses specification-first + iterative refinement.

---

**End of Chapter 10 Lesson Plan**
