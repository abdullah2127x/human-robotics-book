# Chapter 5: High-Fidelity Rendering and Human-Robot Interaction in Unity — Lesson Plan

**Generated by**: chapter-planner v2.0.0 (Reasoning-Activated)
**Source Spec**: specs/book/part-2-spec.md
**Created**: 2025-12-16
**Constitution**: v6.0.1 (Reasoning Mode)

---

## I. Chapter Analysis

### Chapter Type
**Technical/Code-Focused (Hybrid with Visualization)** — Unity-ROS 2 integration, photorealistic rendering, and HRI scripting. Chapter teaches both visualization skills (environment design, rendering) and control skills (C# scripting, ROS 2 messaging). Learning objectives emphasize "set up," "import," "create," "script," "implement" (action verbs). Mix of GUI-based setup and code-based implementation.

### Concept Density Analysis

**Core Concepts** (from spec): 8 major concepts
1. Unity-ROS 2 bridge (ROS-TCP-Connector communication)
2. URDF import to Unity (URDF Importer package)
3. High-fidelity rendering (lighting, materials, shaders)
4. Human avatar animation (Mixamo, animation controllers)
5. Interaction scripting (C# MonoBehaviors, event handling)
6. Scene management (loading, persistence, transitions)
7. Real-time visualization (camera views, UI, viewport management)
8. ROS 2 message publishing/subscribing from Unity (message serialization)

**Complexity Assessment**: **Standard** — 8 concepts, moderate complexity. Concepts build logically: bridge setup → URDF import → environment creation → avatar addition → scripting interactions → ROS integration. No advanced graphics programming required; focus on using pre-built assets and integrating with ROS 2. Prerequisites from Chapter 4 (Gazebo simulation) provide context but aren't hard dependencies.

**Proficiency Tier**: B2 (Intermediate Application) from chapter-index.md
- Requires understanding from Chapter 4 (humanoid model, simulation concepts)
- Demands practical C# programming and Unity editor navigation
- Higher complexity than Chapter 4 due to environment design and animation

**Justified Lesson Count**: 8 lessons
- Layer 1 (Manual): 2 lessons (Unity setup, URDF import basics)
- Layer 2 (AI Collaboration): 3 lessons (environment creation, avatar animation, interaction scripting)
- Layer 3 (Intelligence Design): 2 lessons (reusable interaction patterns, camera control skill)
- Layer 4 (Capstone): 1 lesson (complete HRI scenario with Unity+ROS 2 integration)
- **Total**: 8 lessons

**Reasoning**: Concept density (8 concepts) at B2 tier justifies 8 lessons. Manual foundation handles technical setup (bridge, URDF import); AI collaboration teaches creative/complex tasks (environment design, animation, interaction scripting); Intelligence Design encodes HRI patterns; Layer 4 integrates all into cohesive scenario.

---

## II. Success Criteria (from Part 2 Spec)

### Technical Success Evals
- **Eval-5.1**: Students set up Unity-ROS 2 communication bridge
- **Eval-5.2**: Students import and visualize URDF humanoid in Unity
- **Eval-5.3**: Students create photorealistic indoor environment
- **Eval-5.4**: Students implement human avatar with animations
- **Eval-5.5**: Students script human-robot interaction scenario
- **Eval-5.6**: Students publish Unity events to ROS 2 topics

### Pedagogical Success
- [ ] All lessons follow 4-layer teaching framework
- [ ] Show-then-explain pattern in every lesson
- [ ] "Try With AI" sections demonstrate AI collaboration (no meta-commentary)
- [ ] Exercises have checkbox success criteria
- [ ] Capstone integrates chapter concepts

---

## III. Lesson Sequence

### Lesson 1: Unity Setup and ROS-TCP-Connector Bridge (Layer 1: Manual Foundation)

**Learning Objective**: Install and configure Unity with ROS-TCP-Connector, establishing bidirectional communication with ROS 2 system.

**Stage**: 1 (Manual Foundation)

**CEFR Proficiency**: B2

**New Concepts** (count: 5 — within B2 limit of 7-10):
1. Unity project structure (Assets, Scenes, Scripts folders)
2. ROS-TCP-Connector package (installation, configuration)
3. ROS bridge network communication (IP, port setup)
4. Message serialization (converting ROS 2 messages to Unity objects)
5. Connection validation (monitoring bridge status, message flow)

**Cognitive Load Validation**: 5 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-5.1 (set up ROS 2 bridge)

**Content Elements**:
- **Step-by-step setup**: Install Unity 2022.3 LTS, create project, add ROS-TCP-Connector package
- **Bridge configuration**: Set ROS master IP/port, configure message registry
- **Network topology diagram**: Show host machine, ROS 2 nodes, Unity running separately
- **No AI yet**: Manual setup ensures understanding of bridge mechanics
- **Validation**: Write simple ROS 2 publisher node, verify Unity receives messages
- **Checkpoint**: Students demonstrate working bidirectional communication

**Prerequisites**: Part 1 completion (ROS 2 basics)

**Estimated Time**: 90 minutes

**Teaching Pattern**: Direct tutorial + hands-on configuration (differs from Chapter 4's exploration approach)

---

### Lesson 2: URDF Import and Basic Visualization (Layer 1: Manual Foundation)

**Learning Objective**: Import humanoid URDF into Unity scene and understand model hierarchy, materials, and basic rendering.

**Stage**: 1 (Manual Foundation)

**CEFR Proficiency**: B2

**New Concepts** (count: 5 — within B2 limit):
1. URDF Importer package for Unity
2. Model hierarchy visualization (parent-child links)
3. Material assignment (mesh renderers, physics colliders)
4. Camera positioning and view control
5. Joint visualization in editor vs runtime

**Cognitive Load Validation**: 5 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-5.2 (import and visualize URDF)

**Content Elements**:
- **URDF import workflow**: Add URDF to Assets, use URDF Importer, select import options
- **Model inspection**: Examine hierarchy, identify links/joints, understand parent-child relationships
- **Material basics**: Apply Unity materials to imported meshes (colors, basic shaders)
- **Camera setup**: Position camera for good view of humanoid, configure viewport
- **Runtime testing**: Verify model displays correctly, joints show proper structure
- **No interactivity yet**: Just visualization and understanding model structure

**Prerequisites**: Lesson 1 (ROS bridge setup), Chapter 4 (URDF/humanoid knowledge)

**Estimated Time**: 90 minutes

**Teaching Pattern**: Step-by-step import process + hands-on exploration

---

### Lesson 3: Building Photorealistic Environments with AI Assistance (Layer 2: AI Collaboration)

**Learning Objective**: Create high-fidelity indoor environment (living room, office, or warehouse) using Unity assets and lighting, with AI helping optimize design and performance.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 6 — within B2 limit):
1. Scene composition (organizing objects, using prefabs)
2. Lighting systems (directional, point, spot lights, shadows)
3. Materials and shaders (advanced material properties)
4. Texture mapping and asset selection
5. Performance optimization (draw calls, LOD, culling)
6. Environmental details (plants, furniture, decorations for realism)

**Cognitive Load Validation**: 6 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-5.3 (create photorealistic environment)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student creates environment with poor lighting (looks flat and unrealistic)
   - *AI teaches*: "Three-point lighting pattern creates photorealism: key light (dominant), fill light (shadow detail), back light (separation). Use baked lighting for performance. Here's why each matters..."
   - *Learning*: Student learns professional lighting technique

2. **AI as Student**:
   - *Scenario*: AI suggests ultra-realistic textures everywhere
   - *Student corrects*: "Performance matters. We need real-time interaction. Use lower-resolution textures, optimize draw calls."
   - *AI adapts*: Suggests asset optimization strategy

3. **AI as Co-Worker**:
   - *Scenario*: Balancing realism vs performance
   - *Iteration 1*: High-detail assets → beautiful but slow (30 FPS)
   - *Iteration 2*: Low-detail assets → fast but looks cheap
   - *Iteration 3*: Strategic detail placement (high-detail near camera, low-detail background) + baked lighting
   - *Convergence*: Professional approach combining both constraints

**Content Elements**:
- **Environment choice**: Select scenario (living room, office, warehouse)
- **Layout planning**: Sketch or document room layout before building
- **Asset sourcing**: Use Unity Asset Store or free resources (respect licenses)
- **Lighting design**: Implement three-point lighting with shadows
- **Material application**: Assign realistic materials to objects
- **Performance profiling**: Check frame rate, identify bottlenecks
- **AI collaboration**: Ask AI for layout suggestions, lighting recipes, asset recommendations
- **Testing**: Verify environment loads quickly, renders smoothly, looks photorealistic

**Prerequisites**: Lessons 1-2 (Unity and URDF import)

**Estimated Time**: 120 minutes

---

### Lesson 4: Human Avatar Animation and Character Control (Layer 2: AI Collaboration)

**Learning Objective**: Import human avatar, add animations (walking, gesturing), and control via animation state machine using AI-assisted animation controller setup.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 5 — within B2 limit):
1. Humanoid avatar setup (Mixamo or similar rigged models)
2. Animation controller state machine (Animator, states, transitions)
3. Animation parameters (integers, bools, floats controlling state transitions)
4. Blend trees (smooth animation transitions)
5. Animation events (triggering code at animation keyframes)

**Cognitive Load Validation**: 5 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-5.4 (implement human avatar with animations)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student imports avatar but animations don't play or are jerky
   - *AI teaches*: "Animation state machines require parameter transitions. Set animator parameters (IsWalking, IsGesturing), create states, connect via transitions. Use blend trees for smooth motion. Set animation events for interaction triggers."
   - *Learning*: Student learns animator pattern and best practices

2. **AI as Student**:
   - *Scenario*: AI suggests all animations play simultaneously (confusing)
   - *Student corrects*: "Avatar should have clear states: idle, walking, gesturing. Transitions must be mutually exclusive."
   - *AI adapts*: Creates proper state machine structure

3. **AI as Co-Worker**:
   - *Scenario*: Smooth avatar movement during interaction
   - *Iteration 1*: Simple state machine → jerky transitions
   - *Iteration 2*: Add blend trees → smoother
   - *Iteration 3*: Tune transition timing + animation speed → natural motion
   - *Convergence*: Polished animation system emerges through iteration

**Content Elements**:
- **Avatar import**: Add humanoid character to scene (Mixamo or equivalent)
- **Animation clips**: Import walking, idle, gesture animations
- **Animator setup**: Create Animator controller, define states, parameters
- **State transitions**: Configure when animations change (parameter-based)
- **Blend trees**: Create smooth transitions between movement states
- **Testing**: Walk avatar around environment, trigger gestures, observe fluidity
- **AI collaboration**: Ask AI for animator best practices, animation event patterns

**Prerequisites**: Lessons 1-3 (Unity setup, environment creation)

**Estimated Time**: 120 minutes

---

### Lesson 5: Scripting Human-Robot Interaction (Layer 2: AI Collaboration)

**Learning Objective**: Write C# scripts to implement interaction scenarios where human avatar approaches robot, triggers events, and exchanges information using AI-assisted script development.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 5 — within B2 limit):
1. C# MonoBehaviors (Update, collision detection, events)
2. Raycasting and proximity detection
3. Event system (OnTriggerEnter, triggering custom events)
4. UI system (displaying interaction prompts, feedback)
5. Coroutines (time-based animation sequences)

**Cognitive Load Validation**: 5 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-5.5 (script interaction scenario)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student tries to detect human-robot proximity naively (distance calculation in Update)
   - *AI teaches*: "Use Physics.OverlapSphere for efficient proximity detection. Use OnTriggerEnter for cleaner event handling. Use coroutines for timed animation sequences. This pattern scales better."
   - *Learning*: Student learns performance-aware programming patterns

2. **AI as Student**:
   - *Scenario*: AI suggests overly complex interaction logic
   - *Student clarifies*: "Keep it simple for now: Human approaches, robot acknowledges. Just one interaction."
   - *AI simplifies*: Reduces scope to MVP, provides focused implementation

3. **AI as Co-Worker**:
   - *Scenario*: Responsive interaction that feels natural
   - *Iteration 1*: Instant response → feels robotic
   - *Iteration 2*: Delayed response → feels sluggish
   - *Iteration 3*: Response with animation synchronized to avatar state → natural
   - *Convergence*: Timing and synchronization matter; iteration finds sweet spot

**Content Elements**:
- **Interaction design**: Document desired behavior (approach → acknowledge → disengage)
- **Proximity detection**: Script to detect when human avatar gets close to robot
- **Event triggering**: When proximity detected, trigger robot response
- **Animation control**: Humanoid performs acknowledgment animation (head turn, wave)
- **Feedback UI**: Display interaction status on screen
- **Testing**: Walk avatar toward robot, verify interaction triggers, observe timing
- **AI collaboration**: Ask AI for C# patterns, event handling best practices

**Prerequisites**: Lessons 1-4 (Unity, environment, avatar, animation)

**Estimated Time**: 120 minutes

---

### Lesson 6: ROS 2 Integration and Message Publishing (Layer 2: AI Collaboration)

**Learning Objective**: Connect Unity scripts to ROS 2 system by publishing interaction events and subscribing to robot commands using AI-assisted message serialization and topic configuration.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 4 — within B2 limit):
1. ROS 2 message types and custom messages
2. Publisher setup in Unity (connection, serialization)
3. Subscriber setup for robot state (joint angles, status)
4. Message flow debugging (monitoring topics in ROS 2)

**Cognitive Load Validation**: 4 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-5.6 (publish Unity events to ROS 2)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student tries publishing Unity event to ROS 2 but gets serialization errors
   - *AI teaches*: "ROS messages have strict type contracts. Define message definition (.msg file), register with bridge, ensure field names match exactly. Use ROS-TCP-Connector's message registry pattern."
   - *Learning*: Student learns message serialization requirements

2. **AI as Student**:
   - *Scenario*: AI suggests publishing every frame at high frequency
   - *Student corrects*: "High frequency wastes bandwidth. Only publish on interaction events."
   - *AI adjusts*: Uses event-based publishing instead of frame-based

3. **AI as Co-Worker**:
   - *Scenario*: Keeping Unity and ROS 2 in sync despite network latency
   - *Iteration 1*: Direct messaging → race conditions, occasional misalignment
   - *Iteration 2*: Add acknowledgment protocol → slower but reliable
   - *Iteration 3*: Smart buffering with timeout tolerance → fast AND reliable
   - *Convergence*: Robust communication protocol emerges through iteration

**Content Elements**:
- **Message definition**: Define custom message for interaction events (human position, action)
- **Publisher setup**: Register publisher in ROS bridge, configure topic name
- **Serialization**: Convert C# objects to ROS message format
- **Subscriber setup**: Subscribe to robot state topic (joint feedback)
- **Event binding**: Connect Unity interaction events to ROS publishing
- **Testing**: Trigger interaction, monitor ROS 2 topic to verify publishing
- **Debugging**: Use `ros2 topic echo` to verify message content
- **AI collaboration**: Ask AI for message definition patterns, serialization approaches

**Prerequisites**: Lessons 1-5 (all Unity and interaction setup)

**Estimated Time**: 120 minutes

---

### Lesson 7: Advanced Scene Management and UI Polish (Layer 3: Intelligence Design)

**Learning Objective**: Create reusable HRI scene management skill that encapsulates environment setup, avatar animation, interaction triggering, and ROS integration for reuse across HRI scenarios.

**Stage**: 3 (Intelligence Design)

**CEFR Proficiency**: B2

**Reusable Artifact Created**: **unity-hri-interaction-skill**
- Encapsulates: Scene composition, avatar setup, interaction patterns, ROS bridge orchestration
- Reusable across: Different HRI scenarios, various robot types, simulation/deployment pipeline
- Format: `.claude/skills/unity-hri-interaction/SKILL.md`

**Maps to Evals**: Eval-5.3, Eval-5.4, Eval-5.5, Eval-5.6 (integrative skill)

**Content Elements**:
- **Pattern recognition**: Review Lessons 3-6, identify recurring HRI setup patterns
- **Skill design**: Use Persona + Questions + Principles
  - *Persona*: "Think like a simulation engineer designing reusable HRI test platforms"
  - *Questions*:
    - What environmental elements are essential vs scenario-specific?
    - How do we template avatar behavior for different interaction types?
    - What ROS topics and message contracts must be standardized?
    - How do we make the system testable without ROS 2?
  - *Principles*:
    - Modularity: Environment, avatar, interaction independently configurable
    - Scalability: Support different robot models and interaction types
    - Decoupling: ROS bridge separate from visualization logic for testing
    - Documentation: Clear interface contracts for extending system
- **Implementation**: Document patterns for:
  - Scene template structure (environment prefabs, avatar prefabs)
  - Interaction event system (triggering, sequencing, validation)
  - ROS message flow (publishers, subscribers, topic conventions)
  - Testing without ROS (mock robot control for development)
- **Testing**: Apply skill to set up different HRI scenario (e.g., robot in kitchen)

**Prerequisites**: Lessons 1-6 (all previous)

**Estimated Time**: 90 minutes

---

### Lesson 8: Capstone — Complete HRI Demonstration (Layer 4: Spec-Driven Integration)

**Learning Objective**: Design and implement complete HRI scenario (human avatar interacting with humanoid robot in photorealistic environment) using specification-first approach, composing all chapter learnings and reusable skills.

**Stage**: 4 (Spec-Driven Integration)

**CEFR Proficiency**: B2

**Maps to Evals**: ALL (Eval-5.1 through Eval-5.6 integrated)

**Content Elements**:

1. **Specification Writing** (PRIMARY SKILL — BEFORE ANY IMPLEMENTATION):
   - *Intent*: Specification for complete HRI demo
   - *Constraints*:
     - Human avatar and humanoid robot in same photorealistic scene
     - Human walks toward robot and initiates greeting interaction
     - Robot responds with acknowledgment gesture
     - System maintains bidirectional ROS 2 communication
     - Real-time rendering (60+ FPS on target hardware)
   - *Success criteria*:
     - Human avatar visible and animated walking naturally
     - Robot model visible with proper materials/lighting
     - When human approaches, robot responds (gesture or acknowledgment)
     - ROS 2 topics show interaction events being published
     - No performance degradation during interaction
   - *Acceptance tests*:
     - `test_scene_loads`: Scene renders at 60+ FPS
     - `test_avatar_animation`: Human avatar walks smoothly
     - `test_proximity_detection`: Interaction triggers at correct distance
     - `test_ros_publishing`: ROS 2 topics receive interaction events
     - `test_complete_scenario`: Full interaction sequence executes without errors

2. **Component Composition**:
   - Which skills from Lessons 7 apply?
     - unity-hri-interaction-skill: YES (provides framework)
   - New elements needed:
     - Environment (from Lesson 3)
     - Avatar with animations (from Lesson 4)
     - Interaction scripting (from Lesson 5)
     - ROS integration (from Lesson 6)
   - Architecture: Spec → Environment + Avatar + Robot + Interaction Logic + ROS Bridge

3. **AI Orchestration**:
   - Student provides spec.md to AI
   - AI generates scene setup using unity-hri-interaction-skill
   - Implementation includes all components above
   - Student validates against spec acceptance tests

4. **Iterative Refinement**:
   - Iteration 1: Basic scene with avatars and robot → renders well, no interaction
   - Student feedback: "Add interaction behavior"
   - Iteration 2: Add proximity detection and greeting → works but rough timing
   - Refinement: Synchronize animations with ROS messages
   - Iteration 3: Polish animation timing, optimize performance
   - Result: Polished HRI demo meeting all acceptance tests

5. **Convergence**:
   - Complete HRI system emerges through specification, composition, and iteration
   - Neither student nor AI had full solution at start
   - Combined intelligence produces professional-quality demo

**Prerequisites**: Lessons 1-7 (all foundational through intelligence design)

**Estimated Time**: 150 minutes (1.5 hours teaching + 1.5 hours hands-on)

---

## IV. Skill Dependencies

**Skill Dependency Graph**:
```
ROS Bridge setup (Lesson 1)
    ↓
URDF Import (Lesson 2)
    ↓
Environment Creation (Lesson 3)
    ↓
Avatar Animation (Lesson 4)
    ↓
Interaction Scripting (Lesson 5)
    ↓
ROS Integration (Lesson 6)
    ↓
unity-hri-interaction-skill (Lesson 7)
    ↓
Capstone project (Lesson 8)
```

**Cross-Chapter Dependencies**:
- Chapter 5 assumes: Chapter 4 completion (humanoid URDF, Gazebo simulation understanding)
- Chapter 5 assumes: Part 1 completion (ROS 2 basics)
- Chapter 5 provides: HRI scenarios for Chapter 6 (sensor data visualized in HRI context)

**Validation**:
- ✅ Chapter 4 lessons implemented (provides simulation context)
- ✅ Part 1 chapters complete (provides ROS 2 foundation)
- ✅ Lesson order respects skill dependencies (each builds on previous)

---

## V. Assessment Plan

### Formative Assessments (During Lessons)

- **Lesson 1**: Bidirectional ROS 2 communication verified
- **Lesson 2**: URDF imports, visualizes correctly
- **Lesson 3**: Photorealistic environment with proper lighting
- **Lesson 4**: Avatar with smooth animations, state transitions working
- **Lesson 5**: Interaction triggers when avatar approaches
- **Lesson 6**: ROS 2 topics receive interaction events
- **Lesson 7**: Apply unity-hri-interaction-skill in new HRI scenario

### Summative Assessment (End of Chapter)

- **Lesson 8 Capstone**: Complete HRI demonstration
  - ✅ Spec written first (demonstrates specification skill)
  - ✅ All acceptance tests pass (objective success)
  - ✅ Uses composed components (environment + avatar + interaction + ROS)
  - ✅ Performance meets targets (60+ FPS rendering)
  - ✅ Student can articulate design and integration decisions

**Assessment Alignment**: All assessments align with CEFR B2 proficiency (apply/create cognitive levels), matching Constitution Principle 3.

---

## VI. Validation Checklist

**Chapter-Level Validation**:
- [x] Chapter type identified (Technical/Code-Focused with Visualization)
- [x] Concept density analysis documented (8 concepts → 8 lessons justified)
- [x] Lesson count justified by complexity, not arbitrary
- [x] All evals from spec covered by lessons (Eval-5.1 through Eval-5.6)
- [x] All lessons map to at least one eval

**Stage Progression Validation**:
- [x] Lessons 1-2: Layer 1 (Manual setup and import, no AI)
- [x] Lessons 3-6: Layer 2 (AI Collaboration with Three Roles)
- [x] Lesson 7: Layer 3 (Intelligence Design, reusable skill)
- [x] Lesson 8: Layer 4 (Spec-Driven Integration, capstone)
- [x] Spec-first ONLY in Layer 4

**Cognitive Load Validation**:
- [x] Lesson 1: 5 concepts ≤ 10 limit → ✅
- [x] Lesson 2: 5 concepts ≤ 10 limit → ✅
- [x] Lesson 3: 6 concepts ≤ 10 limit → ✅
- [x] Lesson 4: 5 concepts ≤ 10 limit → ✅
- [x] Lesson 5: 5 concepts ≤ 10 limit → ✅
- [x] Lesson 6: 4 concepts ≤ 10 limit → ✅
- [x] Lesson 7: Intelligence design
- [x] Lesson 8: Capstone integration

**Dependency Validation**:
- [x] Skill dependencies satisfied by lesson order
- [x] Cross-chapter dependencies validated (Chapter 4 → Part 2)

**Teaching Pattern Validation**:
- [x] Lesson 1: Direct tutorial (differs from Chapter 4)
- [x] Lesson 2: Step-by-step import process
- [x] Lessons 3-6: AI Collaboration (Three Roles explicit)
- [x] Lesson 7: Intelligence Design (skill creation)
- [x] Lesson 8: Specification-first capstone

---

## VII. Chapter Success Metrics

**This chapter succeeds when:**
- ✅ ROS 2 to Unity bridge operational and bidirectional (SC-2.3)
- ✅ HRI scenario demonstrates human avatar and robot interaction (SC-2.4)
- ✅ All evaluation criteria (Eval-5.1 through Eval-5.6) achieved by students
- ✅ Reusable HRI skill created (unity-hri-interaction-skill)
- ✅ Capstone delivers professional-quality HRI demonstration with 60+ FPS performance

---

**Chapter 5 Plan — Ready for Content Implementation**

Total estimated content time: 8 lessons × 90-120 minutes = 12-16 hours
Includes: Setup, asset import, environment design, animation control, scripting, ROS integration, capstone
