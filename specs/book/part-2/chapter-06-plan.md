# Chapter 6: Simulating Sensors - LiDAR, Depth Cameras, and IMUs — Lesson Plan

**Generated by**: chapter-planner v2.0.0 (Reasoning-Activated)
**Source Spec**: specs/book/part-2-spec.md
**Created**: 2025-12-16
**Constitution**: v6.0.1 (Reasoning Mode)

---

## I. Chapter Analysis

### Chapter Type
**Technical/Code-Focused** — Gazebo sensor simulation and data processing. Chapter teaches sensor integration (adding sensors to URDF), simulation configuration (plugins), data processing (Python ROS 2 nodes), and visualization. Learning objectives emphasize "add," "configure," "process," "visualize," "record" (action verbs). Practical exercises with real sensor data.

### Concept Density Analysis

**Core Concepts** (from spec): 8 major concepts
1. Sensor plugins architecture (Gazebo sensor system)
2. LiDAR simulation (ray casting, point cloud generation)
3. Depth camera simulation (depth images, RGB-D data)
4. IMU simulation (accelerometer, gyroscope, orientation data)
5. Sensor noise models (Gaussian noise, bias drift)
6. ROS 2 sensor messages (PointCloud2, Image, Imu message types)
7. RViz visualization (displaying sensor data in 3D)
8. ROS 2 bag recording and playback (data capture and replay)

**Complexity Assessment**: **Complex** — 8 concepts, higher complexity than Chapters 4-5. Concepts have interconnections (each sensor type requires plugin → ROS message → visualization → processing workflow). Noise modeling adds mathematical sophistication. Requires understanding from Chapter 4 (Gazebo simulation) and integration with Chapters 4-5.

**Proficiency Tier**: B2 (Intermediate Application) from chapter-index.md
- Advanced sensor data processing
- Understanding of sensor physics (ray casting, depth sensing, inertial measurement)
- Python node development with sensor message handling

**Justified Lesson Count**: 9 lessons
- Layer 1 (Manual): 2 lessons (sensor architecture, individual sensor setup)
- Layer 2 (AI Collaboration): 4 lessons (LiDAR, depth camera, IMU, noise models)
- Layer 3 (Intelligence Design): 2 lessons (reusable sensor integration skill, visualization skill)
- Layer 4 (Capstone): 1 lesson (complete sensor suite with processing pipeline)
- **Total**: 9 lessons

**Reasoning**: Concept density (8 concepts) at B2 tier with high complexity justifies 9 lessons. Manual foundation establishes sensor concepts; 4 AI collaboration lessons teach each sensor type plus noise/processing; 2 Intelligence Design lessons encode sensor and visualization patterns; Layer 4 integrates all into complete sensing system.

---

## II. Success Criteria (from Part 2 Spec)

### Technical Success Evals
- **Eval-6.1**: Students add LiDAR sensor to humanoid URDF
- **Eval-6.2**: Students configure depth camera with correct intrinsics
- **Eval-6.3**: Students add IMU sensor with realistic noise model
- **Eval-6.4**: Students visualize all sensor data in RViz
- **Eval-6.5**: Students process sensor data in Python node
- **Eval-6.6**: Students record and playback sensor data with ROS 2 bags

### Pedagogical Success
- [ ] All lessons follow 4-layer teaching framework
- [ ] Show-then-explain pattern in every lesson
- [ ] "Try With AI" sections demonstrate AI collaboration (no meta-commentary)
- [ ] Exercises have checkbox success criteria
- [ ] Capstone integrates chapter concepts

---

## III. Lesson Sequence

### Lesson 1: Sensor Plugins and Architecture (Layer 1: Manual Foundation)

**Learning Objective**: Understand Gazebo sensor plugin architecture, how sensors integrate with simulation, and data flow to ROS 2.

**Stage**: 1 (Manual Foundation)

**CEFR Proficiency**: B2

**New Concepts** (count: 5 — within B2 limit of 7-10):
1. Gazebo sensor plugin system (sensor element, plugins)
2. Plugin lifecycle (initialization, update, destruction)
3. Data generation vs data transmission (sensor update vs ROS publish)
4. ROS 2 message types for sensors (PointCloud2, Image, Imu)
5. Message publishing patterns (rate limiting, timestamps)

**Cognitive Load Validation**: 5 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-6.1, 6.2, 6.3 (foundational for all sensor work)

**Content Elements**:
- **Sensor architecture diagram**: Show data flow from simulation → plugin → ROS 2 message → visualization
- **URDF sensor elements**: Explain `<sensor>` tags in URDF (type, pose, update_rate)
- **Plugin configuration**: Show SDF world file plugin declarations
- **Message type overview**: Brief introduction to PointCloud2, Image, Imu types
- **No AI yet**: Build foundational understanding manually
- **Checkpoint**: Students explain plugin-to-ROS data flow

**Prerequisites**: Chapter 4 completion (Gazebo fundamentals)

**Estimated Time**: 90 minutes

**Teaching Pattern**: Direct teaching + architecture diagrams (differs from Chapter 5's emphasis on setup)

---

### Lesson 2: Adding Sensors to URDF (Layer 1: Manual Foundation)

**Learning Objective**: Modify humanoid URDF to add sensor definitions for LiDAR, depth camera, and IMU.

**Stage**: 1 (Manual Foundation)

**CEFR Proficiency**: B2

**New Concepts** (count: 5 — within B2 limit):
1. LiDAR sensor definition (horizontal_samples, vertical_samples, range)
2. Depth camera specification (image size, field of view, near/far planes)
3. IMU configuration (accelerometer, gyroscope enabled, measurement range)
4. Sensor frame attachment (which link, position, orientation)
5. Update rates and publishing frequency

**Cognitive Load Validation**: 5 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-6.1, 6.2, 6.3 (enables sensor implementation)

**Content Elements**:
- **URDF sensor syntax**: Show sensor element structure with parameters
- **LiDAR parameters explained**: samples = resolution, range = max distance, noise = uncertainty
- **Depth camera parameters**: image size affects data volume, FOV affects coverage, near/far planes affect range
- **IMU parameters**: Noise scale for realistic measurement error, bias modeling
- **Sensor placement strategy**: LiDAR on head (wide view), depth on head (forward view), IMU on torso (center of mass)
- **No simulation yet**: Just URDF modification and understanding
- **Checkpoint**: Students show URDF with all three sensor types defined

**Prerequisites**: Lesson 1 (sensor architecture), Chapter 4 (URDF skills)

**Estimated Time**: 90 minutes

**Teaching Pattern**: Step-by-step URDF modification, hands-on practice

---

### Lesson 3: LiDAR Simulation with AI Assistance (Layer 2: AI Collaboration)

**Learning Objective**: Configure and tune LiDAR simulation in Gazebo, generating realistic point cloud data with appropriate noise using AI-assisted debugging.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 5 — within B2 limit):
1. Ray casting algorithm (how LiDAR simulation works)
2. Point cloud data structure (PointCloud2 message format, x/y/z coordinates)
3. Horizontal and vertical resolution (angular sampling)
4. Range accuracy and noise (Gaussian noise on depth measurements)
5. Data visualization and interpretation (point clouds in RViz)

**Cognitive Load Validation**: 5 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-6.1 (add LiDAR), Eval-6.4 (visualize in RViz), Eval-6.5 (process data)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student's LiDAR point cloud has poor resolution (too sparse or too dense)
   - *AI teaches*: "Resolution depends on sample count. Too low = sparse, hard to use. Too high = computational cost. Start with 360 samples horizontal, 32 vertical (like real LiDAR). Adjust based on FPS impact."
   - *Learning*: Student learns practical sensor configuration trade-offs

2. **AI as Student**:
   - *Scenario*: AI suggests very high resolution (1000 samples) for accuracy
   - *Student corrects*: "Simulation runs too slow, decreases real-time performance"
   - *AI adapts*: Provides balanced configuration with performance constraints

3. **AI as Co-Worker**:
   - *Scenario*: Detecting obstacles with LiDAR point cloud
   - *Iteration 1*: Raw point cloud used → noisy detection, false positives
   - *Iteration 2*: Add noise filter → cleaner but loses fine details
   - *Iteration 3*: Combine filtering + clustering → robust obstacle detection
   - *Convergence*: Processing pipeline emerges through iteration

**Content Elements**:
- **LiDAR plugin configuration**: Add plugin to world file, set parameters
- **Point cloud topics**: Subscribe to /lidar/points or similar topic
- **RViz visualization**: Display point cloud as 3D scatter, color by range/intensity
- **Data inspection**: Use `ros2 topic echo` to examine point cloud structure
- **Processing basics**: Read point cloud in Python, extract range/angle information
- **Noise addition**: Configure Gaussian noise parameters in plugin
- **Testing**: Simulate LiDAR detecting objects in environment, observe point cloud

**Prerequisites**: Lessons 1-2 (sensor architecture and URDF)

**Estimated Time**: 120 minutes

---

### Lesson 4: Depth Camera Simulation with AI Assistance (Layer 2: AI Collaboration)

**Learning Objective**: Configure depth camera simulation generating RGB-D data (depth images + color images) with AI-assisted intrinsics calibration and processing.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 5 — within B2 limit):
1. Depth image representation (depth values as pixel intensity/distance)
2. RGB image data (color channels, resolution independence)
3. Camera intrinsics (focal length, principal point, distortion)
4. Depth range and noise (near/far planes, measurement error)
5. 3D reconstruction from depth images (projecting 2D images to 3D)

**Cognitive Load Validation**: 5 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-6.2 (configure depth camera), Eval-6.4 (visualize), Eval-6.5 (process)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student's depth camera produces unrealistic data (wrong range or distorted)
   - *AI teaches*: "Depth camera intrinsics matter. Focal length affects depth accuracy at distance. Principal point should be near image center. Distortion parameters affect edge accuracy. Match simulation intrinsics to real camera model."
   - *Learning*: Student learns camera calibration concepts

2. **AI as Student**:
   - *Scenario*: AI suggests high resolution depth (1280x960) for accuracy
   - *Student corrects*: "Real-time processing too slow. Start with 640x480, optimize later"
   - *AI adapts*: Provides reasonable resolution with performance consideration

3. **AI as Co-Worker**:
   - *Scenario*: Extracting 3D objects from depth image
   - *Iteration 1*: Direct depth thresholding → finds floor, not objects
   - *Iteration 2*: Difference from floor plane → better but fragmented
   - *Iteration 3*: Clustering + filtering → robust object detection
   - *Convergence*: Processing strategy emerges from iteration

**Content Elements**:
- **Depth camera plugin configuration**: Plugin setup with intrinsics
- **Sensor topics**: Subscribe to /depth/image and /depth/camera_info topics
- **Image visualization**: Display depth and RGB images in RViz
- **Intrinsics calibration**: Understand focal length, principal point, distortion
- **Depth processing**: Read depth image in Python, extract 3D coordinates
- **Testing**: Simulate depth camera observing environment, visualize output

**Prerequisites**: Lessons 1-3 (sensor architecture, URDF, LiDAR)

**Estimated Time**: 120 minutes

---

### Lesson 5: IMU Simulation with Realistic Noise (Layer 2: AI Collaboration)

**Learning Objective**: Configure IMU sensor with realistic noise models (bias, drift), simulate orientation tracking using AI-assisted tuning.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 5 — within B2 limit):
1. IMU data components (acceleration, angular velocity, magnetic field)
2. Noise models (white noise, bias, random walk)
3. Sensor drift (bias accumulation over time)
4. Orientation representation (quaternion, Euler angles)
5. IMU fusion for orientation estimation (basic kalman filtering concept)

**Cognitive Load Validation**: 5 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-6.3 (add IMU with noise), Eval-6.4 (visualize), Eval-6.5 (process)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student's simulated humanoid falls during movement due to bad IMU data
   - *AI teaches*: "IMU noise affects balance control. Noise too high = controller can't stabilize. Set white noise scale ~0.01 for accel, lower for gyro. Model bias drift for realistic long-term behavior."
   - *Learning*: Student learns tuning parameters for realistic IMU

2. **AI as Student**:
   - *Scenario*: AI suggests no noise (perfect IMU measurements)
   - *Student corrects*: "Real sensors have noise. Without realistic noise, controller won't work on real robot"
   - *AI adjusts*: Adds appropriate noise models

3. **AI as Co-Worker**:
   - *Scenario*: Reliable orientation estimation despite noisy IMU
   - *Iteration 1*: Raw IMU angles → drifts over time
   - *Iteration 2*: Add bias compensation → helps but still drifts
   - *Iteration 3*: Kalman filter fusing IMU with other sensors → stable orientation
   - *Convergence*: Sensor fusion approach better than raw data

**Content Elements**:
- **IMU plugin configuration**: Accelerometer and gyroscope settings
- **Noise model parameters**: White noise scale, bias variance, bias stability
- **IMU topics**: Subscribe to /imu/data_raw or /imu/data (filtered)
- **Quaternion representation**: Understand attitude representation from IMU
- **Data visualization**: Display orientation in RViz (TF frames)
- **Processing in Python**: Read IMU, extract acceleration and angular velocity, estimate orientation
- **Testing**: Move humanoid, observe IMU measurements, verify realistic values

**Prerequisites**: Lessons 1-4 (sensor architecture, URDF, LiDAR, depth)

**Estimated Time**: 120 minutes

---

### Lesson 6: Sensor Noise Models and Realistic Data (Layer 2: AI Collaboration)

**Learning Objective**: Configure and understand sensor noise models to produce realistic data suitable for algorithm validation, using AI for debugging and tuning.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 4 — within B2 limit):
1. Gaussian noise and signal-to-noise ratio (SNR)
2. Bias and drift in measurements
3. Outlier generation (transient noise spikes)
4. Noise validation (comparing simulation to real sensor specs)

**Cognitive Load Validation**: 4 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-6.1, 6.2, 6.3 (all sensors with noise), Eval-6.5 (processing noisy data)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student's sensor noise is unrealistic (too high or not matching real sensor specs)
   - *AI teaches*: "Real sensors have documented noise specifications. LiDAR: ±0.02m at 10m. Depth: ±0.05m at 1m. Match simulation noise to spec sheets. Too much noise makes algorithms fail."
   - *Learning*: Student learns to validate simulation realism

2. **AI as Student**:
   - *Scenario*: AI suggests removing noise for "cleaner" simulation
   - *Student corrects*: "Algorithms must handle realistic noise or fail on real robots"
   - *AI keeps noise, adjusts level appropriately

3. **AI as Co-Worker**:
   - *Scenario*: Balancing realism with algorithm testability
   - *Iteration 1*: Perfect noise-free data → algorithm works perfectly, unrealistic
   - *Iteration 2*: Excessive noise → algorithm fails, too pessimistic
   - *Iteration 3*: Calibrated noise matching spec sheets → realistic and challenging
   - *Convergence*: Appropriate noise level found through validation

**Content Elements**:
- **Noise model types**: White Gaussian noise, bias, random walk
- **Parameter tuning**: Noise scale, bias variance, update frequency effects
- **Validation approach**: Compare simulation data to real sensor measurements
- **Visualizing noise**: Show raw data, filtered data, effect on downstream algorithms
- **Testing with noisy data**: Verify algorithms work with realistic noise
- **Statistical analysis**: Mean, variance, outlier frequency

**Prerequisites**: Lessons 1-5 (all sensor types)

**Estimated Time**: 120 minutes

---

### Lesson 7: RViz Visualization and Sensor Data Analysis (Layer 3: Intelligence Design)

**Learning Objective**: Create reusable RViz configuration and visualization skill for displaying all sensor types comprehensively, enabling data analysis and debugging.

**Stage**: 3 (Intelligence Design)

**CEFR Proficiency**: B2

**Reusable Artifact Created**: **gazebo-sensor-visualization-skill**
- Encapsulates: RViz setup patterns, visualization plugins, color schemes, display configurations
- Reusable across: All sensor chapters, future perception modules
- Format: `.claude/skills/gazebo-sensor-visualization/SKILL.md`

**Maps to Evals**: Eval-6.4 (visualize sensor data)

**Content Elements**:
- **Pattern recognition**: Review Lessons 3-6, identify visualization patterns for each sensor type
- **Skill design**: Use Persona + Questions + Principles
  - *Persona*: "Think like a robotics engineer debugging perception systems with RViz"
  - *Questions*:
    - How do we display different data types simultaneously?
    - What color schemes improve perception of sensor data?
    - How do we layer visualization (raw sensor + processed + detections)?
    - What visualization helps debug noise and filtering issues?
  - *Principles*:
    - Clarity: Each sensor type clearly distinguishable
    - Debugging support: Show both raw and processed data
    - Performance: Don't overwhelm with too many visualizations
    - Consistency: Standard color/symbol conventions across projects
- **Implementation**: Document patterns for:
  - Point cloud display (LiDAR, colored by range/intensity/height)
  - Image display (depth and RGB side-by-side)
  - IMU visualization (orientation axes, acceleration vectors)
  - TF frames for coordinate systems
  - Custom markers for processing results (detected objects, trajectories)
- **Configuration templates**: Provide RViz config files for standard setups
- **Testing**: Apply skill to visualize complete sensor suite from Lesson 6

**Prerequisites**: Lessons 3-6 (all sensor types)

**Estimated Time**: 90 minutes

---

### Lesson 8: Sensor Data Processing in ROS 2 (Layer 3: Intelligence Design)

**Learning Objective**: Create reusable sensor data processing skill encapsulating patterns for extracting meaningful information from raw sensor data (obstacle detection, orientation estimation, etc.).

**Stage**: 3 (Intelligence Design)

**CEFR Proficiency**: B2

**Reusable Artifact Created**: **gazebo-sensor-processing-skill**
- Encapsulates: Processing algorithms, message handling patterns, data fusion approaches
- Reusable across: All perception-based chapters
- Format: `.claude/skills/gazebo-sensor-processing/SKILL.md`

**Maps to Evals**: Eval-6.5 (process sensor data)

**Content Elements**:
- **Pattern identification**: Review Lessons 3-6, identify data processing patterns
- **Skill design**: Use Persona + Questions + Principles
  - *Persona*: "Think like a robotics engineer building robust perception pipelines"
  - *Questions*:
    - How do we handle sensor message timing and buffering?
    - What filtering/smoothing improves noisy sensor data?
    - How do we fuse multiple sensors for better estimates?
    - What validation ensures processed data is correct?
  - *Principles*:
    - Robustness: Handle outliers and missing data gracefully
    - Timeliness: Process data at sensor rate without lag
    - Validation: Sanity checks on outputs
    - Composability: Processing nodes can be chained
- **Implementation**: Document patterns for:
  - LiDAR processing: Point cloud filtering, clustering, obstacle detection
  - Depth camera processing: 3D reconstruction, object detection, distance measurement
  - IMU processing: Noise filtering, bias compensation, orientation fusion
  - Multi-sensor fusion: Combining LiDAR and depth for robust perception
  - Data validation: Checking for sensor failures or anomalies
- **Python ROS 2 patterns**: Subscriber callbacks, message handling, timer-based processing
- **Testing**: Apply skill to process all three sensor types simultaneously

**Prerequisites**: Lessons 3-7 (all sensors and visualization)

**Estimated Time**: 90 minutes

---

### Lesson 9: Capstone — Complete Sensor Suite with Processing Pipeline (Layer 4: Spec-Driven Integration)

**Learning Objective**: Design and implement humanoid with full sensor suite (LiDAR, depth camera, IMU) publishing realistic data and processing for obstacle detection using specification-first approach.

**Stage**: 4 (Spec-Driven Integration)

**CEFR Proficiency**: B2

**Maps to Evals**: ALL (Eval-6.1 through Eval-6.6 integrated)

**Content Elements**:

1. **Specification Writing** (PRIMARY SKILL — BEFORE ANY CODE):
   - *Intent*: Specification for humanoid with complete sensor suite and processing
   - *Constraints*:
     - Humanoid equipped with LiDAR, depth camera, and IMU
     - All sensors publishing realistic data with appropriate noise models
     - Processing nodes detecting obstacles and estimating orientation
     - System records sensor data to ROS 2 bags for playback and analysis
     - Real-time performance maintained (minimum 20 Hz sensor updates)
   - *Success criteria*:
     - All three sensor types publishing continuously
     - Sensor data visualized correctly in RViz
     - Obstacle detection working (LiDAR and/or depth)
     - Orientation estimation from IMU reasonable
     - Sensor data recording to bag file
   - *Acceptance tests*:
     - `test_lidar_publishing`: Point clouds at 10+ Hz
     - `test_depth_publishing`: Depth images at 30+ Hz
     - `test_imu_publishing`: IMU at 100+ Hz
     - `test_obstacle_detection`: Correctly identifies objects near humanoid
     - `test_orientation_estimate`: IMU-based orientation reasonable
     - `test_bag_recording`: Data recorded to bag file, playable

2. **Component Composition**:
   - Which skills from Lessons 7-8 apply?
     - gazebo-sensor-visualization-skill: YES (visualize all sensors)
     - gazebo-sensor-processing-skill: YES (process sensor data)
   - New components needed:
     - Obstacle detection node (process LiDAR/depth data)
     - Orientation estimator (process IMU data)
     - Bag recording launcher
   - Architecture: Humanoid with sensors → ROS topics → Processing nodes → Visualization + Recording

3. **AI Orchestration**:
   - Student provides spec.md to AI
   - AI generates sensor configuration and processing pipeline
   - Uses composed skills for visualization and processing
   - Student validates against acceptance tests

4. **Iterative Refinement**:
   - Iteration 1: All sensors publishing data, visualized in RViz
     - Result: Raw sensor data working (Evals 6.1-6.4 pass)
     - Issue: No processing/interpretation of data
   - Iteration 2: Add obstacle detection processing
     - Result: Can detect objects near humanoid
     - Issue: Detection occasionally fails or has false positives
   - Iteration 3: Improve detection robustness (filtering, noise handling)
     - Add IMU-based orientation estimation
     - Implement bag recording
     - Result: All acceptance tests pass
   - Convergence: Complete perception system emerges through iteration

5. **Capstone Validation**:
   - Move humanoid around environment while sensors collect data
   - Verify obstacle detection works during movement
   - Verify orientation estimation matches actual orientation
   - Record data to bag file, playback and analyze
   - All evals achieved

**Prerequisites**: Lessons 1-8 (all sensor fundamentals and processing)

**Estimated Time**: 150 minutes (1.5 hours teaching + 1.5 hours hands-on)

---

## IV. Skill Dependencies

**Skill Dependency Graph**:
```
Sensor architecture (Lesson 1)
    ↓
URDF sensor definitions (Lesson 2)
    ↓
LiDAR simulation (Lesson 3)
    ├─→ Depth camera simulation (Lesson 4)
    │   ├─→ IMU simulation (Lesson 5)
    │   └─→ Noise models (Lesson 6)
    ├─→ gazebo-sensor-visualization-skill (Lesson 7)
    └─→ gazebo-sensor-processing-skill (Lesson 8)
        ↓
    Capstone project (Lesson 9)
```

**Cross-Chapter Dependencies**:
- Chapter 6 assumes: Chapter 4 completion (Gazebo simulation, humanoid URDF)
- Chapter 6 assumes: Part 1 completion (ROS 2 nodes, Python rclpy)
- Chapter 6 assumes: Chapter 5 beneficial but not required (visualization patterns)
- Chapter 6 provides: Perception foundation for Part 3 (VSLAM, navigation using sensor data)

**Validation**:
- ✅ Chapter 4 chapters complete (Gazebo foundation)
- ✅ Part 1 chapters complete (ROS 2 foundation)
- ✅ Lesson order respects sensor dependencies (architecture → individual sensors → processing)

---

## V. Assessment Plan

### Formative Assessments (During Lessons)

- **Lesson 1**: Explain sensor plugin architecture
- **Lesson 2**: URDF with all three sensor types defined
- **Lesson 3**: LiDAR publishing realistic point cloud data
- **Lesson 4**: Depth camera publishing RGB-D images
- **Lesson 5**: IMU publishing with realistic noise model
- **Lesson 6**: All sensors tuned for realistic simulation
- **Lesson 7**: Visualize all sensor data types in RViz
- **Lesson 8**: Process sensor data, detect obstacles and estimate orientation

### Summative Assessment (End of Chapter)

- **Lesson 9 Capstone**: Complete sensor suite with processing pipeline
  - ✅ Spec written first (demonstrates specification skill)
  - ✅ All acceptance tests pass (all three sensors, processing, recording)
  - ✅ Uses composed skills (visualization, processing)
  - ✅ Student can articulate processing decisions and sensor trade-offs

**Assessment Alignment**: All assessments align with CEFR B2 proficiency (apply/analyze/create cognitive levels), matching Constitution Principle 3.

---

## VI. Validation Checklist

**Chapter-Level Validation**:
- [x] Chapter type identified (Technical/Code-Focused)
- [x] Concept density analysis documented (8 concepts → 9 lessons justified)
- [x] Lesson count justified by complexity, not arbitrary
- [x] All evals from spec covered by lessons (Eval-6.1 through Eval-6.6)
- [x] All lessons map to at least one eval

**Stage Progression Validation**:
- [x] Lessons 1-2: Layer 1 (Manual foundation, no AI)
- [x] Lessons 3-6: Layer 2 (AI Collaboration with Three Roles)
- [x] Lessons 7-8: Layer 3 (Intelligence Design, reusable skills)
- [x] Lesson 9: Layer 4 (Spec-Driven Integration, capstone)
- [x] Spec-first ONLY in Layer 4

**Cognitive Load Validation**:
- [x] Lesson 1: 5 concepts ≤ 10 limit → ✅
- [x] Lesson 2: 5 concepts ≤ 10 limit → ✅
- [x] Lesson 3: 5 concepts ≤ 10 limit → ✅
- [x] Lesson 4: 5 concepts ≤ 10 limit → ✅
- [x] Lesson 5: 5 concepts ≤ 10 limit → ✅
- [x] Lesson 6: 4 concepts ≤ 10 limit → ✅
- [x] Lesson 7: Intelligence design
- [x] Lesson 8: Intelligence design
- [x] Lesson 9: Capstone integration

**Dependency Validation**:
- [x] Skill dependencies satisfied by lesson order
- [x] Cross-chapter dependencies validated (Chapter 4 → Part 2)

**Teaching Pattern Validation**:
- [x] Lesson 1: Direct teaching + architecture (differs from Chapters 4-5)
- [x] Lesson 2: Step-by-step URDF modification
- [x] Lessons 3-6: AI Collaboration (Three Roles explicit)
- [x] Lessons 7-8: Intelligence Design (skill creation)
- [x] Lesson 9: Specification-first capstone

---

## VII. Chapter Success Metrics

**This chapter succeeds when:**
- ✅ All three sensor types (LiDAR, depth, IMU) integrated into humanoid
- ✅ Sensor data publishing to ROS 2 with realistic noise
- ✅ All sensor data visualized correctly in RViz (SC-2.6)
- ✅ Processing pipeline detecting obstacles and estimating orientation (SC-2.5)
- ✅ Sensor data recording and playback functional (SC-2.7)
- ✅ All evaluation criteria (Eval-6.1 through Eval-6.6) achieved by students
- ✅ Reusable skills created (visualization, processing)
- ✅ Capstone demonstrates complete perception system

---

**Chapter 6 Plan — Ready for Content Implementation**

Total estimated content time: 9 lessons × 90-120 minutes = 13.5-18 hours
Includes: Sensor architecture, URDF modification, LiDAR, depth camera, IMU, noise modeling, visualization, processing, capstone
