# Chapter 7: NVIDIA Isaac Sim - Photorealistic Simulation and Synthetic Data — Lesson Plan

**Generated by**: chapter-planner v2.0.0 (Reasoning-Activated)
**Source Spec**: specs/book/part-3-spec.md
**Created**: 2025-12-17
**Constitution**: v6.0.1 (Reasoning Mode)

---

## I. Chapter Analysis

### Chapter Type
**Technical/Code-Focused** — Isaac Sim simulation and synthetic data generation. Chapter teaches implementation skills (installation, URDF import, domain randomization, Replicator API usage, ROS 2 bridge configuration). Learning objectives emphasize "install," "create," "implement," "configure," "generate" (action verbs indicating hands-on technical skills). Code examples and practical exercises required throughout.

### Concept Density Analysis

**Core Concepts** (from spec): 10 major concepts
1. Isaac Sim architecture (Omniverse Kit, PhysX 5, RTX rendering)
2. Omniverse Nucleus (asset management, collaboration)
3. USD (Universal Scene Description) format
4. Photorealistic rendering (RTX ray tracing, materials, lighting)
5. Domain randomization (texture, lighting, pose variation)
6. Replicator (synthetic data generation API)
7. Isaac Sim sensors (cameras, LiDAR with GPU acceleration)
8. ROS 2 bridge in Isaac Sim
9. Performance optimization (LOD, culling, render settings)
10. Isaac Sim vs Gazebo tradeoffs

**Complexity Assessment**: **Complex** — 10 concepts, significant complexity. Isaac Sim is a comprehensive GPU-accelerated simulation platform with steep learning curve. Concepts span multiple domains: graphics (RTX, USD), physics (PhysX 5), AI (domain randomization, synthetic data), and robotics (ROS 2 integration). Prerequisites from Part 1-2 (URDF, ROS 2, Gazebo) provide foundation but GPU acceleration and Omniverse ecosystem are new paradigms.

**Proficiency Tier**: B2 (Intermediate Application) from chapter-index.md
- Students can apply configuration patterns with moderate guidance
- Expected to troubleshoot and customize independently
- GPU acceleration concepts require systems-level thinking

**Justified Lesson Count**: 9 lessons
- Layer 1 (Manual): 2 lessons (installation/setup, Isaac Sim architecture)
- Layer 2 (AI Collaboration): 4 lessons (URDF import, rendering, domain randomization, ROS 2 bridge)
- Layer 3 (Intelligence Design): 2 lessons (domain-randomization skill, performance-optimization skill)
- Layer 4 (Capstone): 1 lesson (synthetic training dataset generation project)
- **Total**: 9 lessons

**Reasoning**: High concept density (10 concepts) at B2 proficiency justifies 9 lessons. Complex tooling (Isaac Sim ecosystem) requires substantial Layer 1 foundation (2 lessons). Domain randomization and Replicator API are multi-faceted (justify 2 separate Layer 2 lessons). Performance optimization and GPU considerations warrant dedicated coverage. Two Layer 3 lessons encode reusable patterns for domain randomization and performance tuning. Capstone integrates all concepts into production-ready synthetic data pipeline.

---

## II. Success Criteria (from Part 3 Spec)

### Technical Success Evals
- **Eval-7.1**: Students install Isaac Sim and verify GPU acceleration
- **Eval-7.2**: Students import humanoid URDF into Isaac Sim scene
- **Eval-7.3**: Students create photorealistic indoor environment with proper lighting
- **Eval-7.4**: Students implement domain randomization for training data diversity
- **Eval-7.5**: Students generate synthetic dataset (10,000+ images) using Replicator
- **Eval-7.6**: Students configure Isaac Sim ROS 2 bridge for sensor data publishing
- **Eval-7.7**: Students analyze Isaac vs Gazebo for specific use cases

### Pedagogical Success
- [ ] All lessons follow 4-layer teaching framework
- [ ] Hands-on discovery pattern (students explore Isaac Sim interface)
- [ ] Socratic dialogue for tool tradeoff analysis (Isaac vs Gazebo)
- [ ] "Try With AI" sections demonstrate AI collaboration (no meta-commentary)
- [ ] Exercises have checkbox success criteria
- [ ] Capstone integrates all chapter concepts

---

## III. Lesson Sequence

### Lesson 1: Installing Isaac Sim and Understanding the Ecosystem (Layer 1: Manual Foundation)

**Learning Objective**: Install NVIDIA Isaac Sim via Omniverse Launcher, verify GPU acceleration, and understand the Omniverse ecosystem architecture by manual exploration.

**Stage**: 1 (Manual Foundation)

**CEFR Proficiency**: B2

**New Concepts** (count: 7 — within B2 limit of 7-10):
1. Omniverse Launcher (application hub for NVIDIA tools)
2. Isaac Sim application structure (standalone vs embedded)
3. NVIDIA GPU driver requirements (535+ for CUDA 12.x)
4. RTX rendering pipeline (ray tracing, rasterization fallback)
5. Omniverse Nucleus (local vs cloud asset storage)
6. License activation (free for education/personal use)
7. System requirements validation (GPU VRAM, RAM, disk space)

**Cognitive Load Validation**: 7 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-7.1 (install and verify GPU acceleration)

**Content Elements**:
- **Prerequisites check**: Students verify NVIDIA GPU (RTX 3070+), Ubuntu 22.04, 32GB+ RAM
- **Manual installation walkthrough**: Omniverse Launcher download → Isaac Sim install → GPU driver verification
- **Step-by-step validation**: Run sample scene, check GPU utilization (nvidia-smi), verify RTX rendering active
- **Architecture exploration**: Students examine Isaac Sim interface manually, identify key panels (viewport, content browser, property panel, stage)
- **No AI assistance yet**: Build mental model of ecosystem before AI collaboration
- **Troubleshooting section**: Common installation issues (driver mismatch, VRAM insufficient, Vulkan errors)
- **Checkpoint**: Students successfully launch Isaac Sim, run sample humanoid scene, confirm GPU rendering

**Prerequisites**: Part 1-2 completion (ROS 2, URDF, Gazebo), NVIDIA GPU hardware

**Estimated Time**: 120 minutes (installation can be time-consuming)

**Teaching Pattern**: Hands-on discovery (students explore interface, learn through experimentation)

---

### Lesson 2: Isaac Sim Architecture and USD Format (Layer 1: Manual Foundation)

**Learning Objective**: Understand Isaac Sim's architecture (Omniverse Kit, PhysX 5, RTX rendering pipeline) and USD format fundamentals by analyzing and modifying scene files manually.

**Stage**: 1 (Manual Foundation)

**CEFR Proficiency**: B2

**New Concepts** (count: 8 — within B2 limit):
1. Omniverse Kit extensions (modular architecture)
2. PhysX 5 physics engine (vs ODE in Gazebo)
3. USD (Universal Scene Description) file format
4. Stage hierarchy (prims, attributes, relationships)
5. USD layers (composition, overrides, variants)
6. Isaac Sim API structure (Python omni.isaac.core)
7. Rendering pipeline (RTX vs rasterization modes)
8. Asset management (Nucleus paths, local vs remote assets)

**Cognitive Load Validation**: 8 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-7.2 (foundational for URDF import), Eval-7.7 (understand architecture for comparison)

**Content Elements**:
- **Architecture diagram**: Visual explanation of Isaac Sim components (Kit → Extensions → PhysX/RTX/ROS)
- **Manual USD exploration**: Students open .usd file in text editor, understand XML-like structure
- **Stage hierarchy walkthrough**: Students navigate stage tree in Isaac Sim UI, understand prim relationships
- **USD vs SDF comparison**: Socratic questions comparing Gazebo's SDF to Isaac's USD (when to use each?)
- **PhysX 5 introduction**: Compare PhysX articulations to Gazebo's joint controllers
- **No AI yet**: Students build understanding of "why USD" and "why PhysX 5" before using them
- **Practice**: Students modify simple USD scene (change object position, add lighting), reload in Isaac Sim

**Prerequisites**: Lesson 1 (Isaac Sim installed)

**Estimated Time**: 90 minutes

**Teaching Pattern**: Socratic dialogue (comparing Isaac vs Gazebo architecture, forcing reasoning about tool selection)

---

### Lesson 3: Importing URDF Humanoids into Isaac Sim (Layer 2: AI Collaboration)

**Learning Objective**: Import URDF humanoid models into Isaac Sim scenes, validate physics behavior, and troubleshoot import issues with AI assistance.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 5 — within B2 limit):
1. URDF → USD conversion process (Isaac Sim importer)
2. Articulation root configuration (fixed base vs floating)
3. Joint drive parameters (stiffness, damping for Isaac)
4. Collision mesh import validation
5. Material assignment (visual vs collision geometry)

**Cognitive Load Validation**: 5 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-7.2 (import humanoid URDF)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student imports URDF, humanoid collapses on spawn
   - *AI teaches*: "The URDF joint limits aren't mapping correctly to PhysX articulation. In Isaac Sim, you need to configure joint drives with explicit stiffness/damping. Here's the pattern for humanoid joints: [shows configuration]"
   - *Learning*: Student learns PhysX joint parameters differ from Gazebo (requires explicit drive settings)

2. **AI as Student**:
   - *Scenario*: AI suggests default material for all links (generic plastic)
   - *Student teaches*: "Our humanoid has metal legs and rubber feet. Materials matter for friction. Here's the constraint: [specifies material properties]"
   - *Adaptation*: AI generates material assignment code respecting student's domain requirements

3. **AI as Co-Worker**:
   - *Scenario*: Student and AI iterate on collision mesh resolution
   - *Iteration 1*: Student uses high-res collision mesh → simulation slows
   - *Iteration 2*: AI suggests simplified mesh → loses accuracy
   - *Iteration 3*: Together identify using convex decomposition → optimal balance
   - *Convergence*: Neither knew optimal solution initially; collaboration produced better result

**Content Elements**:
- **Recap Layer 1**: Review URDF structure from Part 1, USD architecture from Lesson 2
- **Manual import attempt**: Student uses Isaac Sim URDF importer (will encounter specific issues)
- **AI collaboration**: Show all three roles above
- **Practice exercise**: Students import custom humanoid with AI assistance, validate physics
- **Validation checkpoint**: Humanoid stands stable, joints move correctly, collision detection works

**Prerequisites**: Lessons 1-2 (Isaac Sim fundamentals), Part 1 (URDF)

**Estimated Time**: 120 minutes

---

### Lesson 4: Photorealistic Rendering with RTX (Layer 2: AI Collaboration)

**Learning Objective**: Create photorealistic indoor environments using RTX ray tracing, PBR materials, and lighting systems with AI collaboration for material selection and scene composition.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 7 — within B2 limit of 7-10):
1. PBR (Physically-Based Rendering) materials (albedo, roughness, metallic)
2. RTX ray tracing settings (samples, bounces, denoising)
3. Lighting types (HDRI, dome lights, area lights, point lights)
4. Shadow quality parameters (shadow map resolution, soft shadows)
5. Render quality presets (real-time vs offline rendering)
6. Material library (Omniverse asset browser)
7. Scene composition (camera placement, framing, composition rules)

**Cognitive Load Validation**: 7 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-7.3 (create photorealistic environment)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student creates scene with flat lighting (no shadows)
   - *AI teaches*: "For training data, you need varied lighting conditions. HDRI dome lights provide realistic environment lighting. Here's why: [explains indirect illumination, ambient occlusion]"
   - *Learning*: Student learns HDRI lighting creates realism AI models need for generalization

2. **AI as Student**:
   - *Scenario*: AI suggests maximum ray tracing quality (512 samples, 8 bounces)
   - *Student teaches*: "We need real-time rendering for data generation (30 FPS minimum). Here's the performance constraint: [specifies render budget]"
   - *Adaptation*: AI adjusts settings to balance quality vs speed (64 samples, 2 bounces)

3. **AI as Co-Worker**:
   - *Scenario*: Student and AI iterate on material realism
   - *Iteration 1*: Student uses default materials → too uniform
   - *Iteration 2*: AI suggests random materials → looks artificial
   - *Iteration 3*: Together identify using material categories (wood floors, painted walls, metal fixtures) → realistic variation
   - *Convergence*: Collaboration produced materially-coherent scene

**Content Elements**:
- **Recap Layer 1**: Review RTX rendering pipeline from Lesson 2
- **Manual scene creation**: Student builds basic indoor room (walls, floor, ceiling)
- **AI collaboration**: Material selection, lighting setup, render tuning (show all three roles)
- **Practice exercise**: Students create photorealistic office/warehouse environment with AI
- **Quality validation**: Renders look photorealistic, shadows accurate, materials believable

**Prerequisites**: Lessons 1-3 (Isaac Sim fundamentals, URDF import)

**Estimated Time**: 120 minutes

---

### Lesson 5: Domain Randomization Fundamentals (Layer 2: AI Collaboration)

**Learning Objective**: Implement domain randomization for texture, lighting, and pose variation to increase training data diversity using Isaac Sim Randomization API with AI collaboration.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 8 — within B2 limit of 7-10):
1. Domain randomization theory (why variation improves model robustness)
2. Texture randomization (material swapping, color variation)
3. Lighting randomization (intensity, color temperature, position)
4. Pose randomization (object placement, orientation)
5. Isaac Sim Randomization API (randomizer extensions)
6. Randomization distributions (uniform, normal, categorical)
7. Randomization schedules (per-frame, per-episode)
8. Correlation management (avoid unrealistic combinations)

**Cognitive Load Validation**: 8 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-7.4 (implement domain randomization)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student randomizes textures independently, lighting independently
   - *AI teaches*: "Uncorrelated randomization creates unrealistic scenes (dark wood floor with bright sun, or white walls with dim light). You need correlated randomization. Here's the pattern: [shows lighting-material correlation]"
   - *Learning*: Student learns randomization realism requires managing correlations

2. **AI as Student**:
   - *Scenario*: AI suggests extreme randomization (random colors every pixel)
   - *Student teaches*: "We're training humanoid navigation, not texture recognition. Variation should stay within plausible indoor materials. Here's the constraint: [specifies material categories]"
   - *Adaptation*: AI constrains randomization to realistic material palette

3. **AI as Co-Worker**:
   - *Scenario*: Student and AI iterate on randomization frequency
   - *Iteration 1*: Student randomizes every frame → too much variation (motion blur artifacts)
   - *Iteration 2*: AI suggests randomize per episode → too little variation
   - *Iteration 3*: Together identify randomizing every 10 frames → optimal balance
   - *Convergence*: Collaboration found frequency that provides variation without artifacts

**Content Elements**:
- **Domain randomization motivation**: Why does variation improve model generalization?
- **Manual randomization**: Student writes Python script using Randomization API
- **AI collaboration**: Refine randomization strategy (show all three roles)
- **Practice exercise**: Students implement texture + lighting + pose randomization with AI
- **Validation**: Students generate 100 images, verify variation is realistic and diverse

**Prerequisites**: Lessons 1-4 (Isaac Sim rendering)

**Estimated Time**: 120 minutes

---

### Lesson 6: Generating Synthetic Data with Replicator (Layer 2: AI Collaboration)

**Learning Objective**: Generate large-scale synthetic datasets (10,000+ images) using Isaac Sim Replicator API, with AI collaboration for annotation pipeline and data export configuration.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 9 — within B2 limit of 7-10):
1. Replicator API architecture (graph-based data generation)
2. Replicator writers (output formats: RGB, depth, semantic segmentation, bounding boxes)
3. Annotation automation (ground truth generation)
4. Batch generation (parallel rendering)
5. Data pipeline configuration (camera trajectories, capture triggers)
6. Output formats (COCO, YOLO, custom JSON)
7. Quality validation (annotation accuracy, image diversity)
8. Performance optimization (GPU memory management, render queue)
9. Dataset versioning and organization

**Cognitive Load Validation**: 9 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT (high complexity justified by B2 tier)

**Maps to Evals**: Eval-7.5 (generate synthetic dataset 10,000+ images)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student generates RGB images only, no annotations
   - *AI teaches*: "For object detection training, you need bounding boxes and semantic segmentation. Replicator can generate these automatically. Here's the pattern: [shows writer configuration]"
   - *Learning*: Student learns annotation automation saves manual labeling effort

2. **AI as Student**:
   - *Scenario*: AI suggests COCO format for all annotations
   - *Student teaches*: "Our training pipeline uses YOLO format. Here's the output constraint: [specifies YOLO bounding box format]"
   - *Adaptation*: AI configures Replicator writer for YOLO format

3. **AI as Co-Worker**:
   - *Scenario*: Student and AI iterate on batch size for performance
   - *Iteration 1*: Student uses batch size 1 → too slow (1 image/sec)
   - *Iteration 2*: AI suggests batch size 100 → GPU OOM (out of memory)
   - *Iteration 3*: Together identify optimal batch size based on GPU VRAM → efficient pipeline
   - *Convergence*: Collaboration tuned for hardware constraints

**Content Elements**:
- **Replicator introduction**: Graph-based data generation architecture
- **Manual pipeline setup**: Student writes Replicator script (Python API)
- **AI collaboration**: Configure writers, optimize performance (show all three roles)
- **Practice exercise**: Students generate 1,000 image dataset with annotations
- **Validation**: Dataset has diverse images, annotations accurate, performance acceptable

**Prerequisites**: Lessons 1-5 (Isaac Sim, domain randomization)

**Estimated Time**: 120 minutes

---

### Lesson 7: Configuring Isaac Sim ROS 2 Bridge (Layer 2: AI Collaboration)

**Learning Objective**: Configure Isaac Sim ROS 2 bridge for sensor data publishing (cameras, LiDAR), validate data integrity, and integrate with ROS 2 ecosystem using AI collaboration for bridge configuration.

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B2

**New Concepts** (count: 6 — within B2 limit of 7-10):
1. Isaac Sim ROS 2 bridge architecture (OmniGraph-based)
2. Sensor publishing configuration (topic names, message types, QoS)
3. ROS 2 action server integration (robot control from ROS 2)
4. Coordinate frame transforms (Isaac USD → ROS 2 tf)
5. Clock synchronization (simulation time vs wall time)
6. Bridge performance optimization (publish rate, message throttling)

**Cognitive Load Validation**: 6 concepts ≤ 10 limit (B2) → ✅ WITHIN LIMIT

**Maps to Evals**: Eval-7.6 (configure ROS 2 bridge for sensor publishing)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - *Scenario*: Student publishes camera at 60 Hz, ROS 2 nodes lag
   - *AI teaches*: "High frequency publishing can overload ROS 2. For computer vision, 30 Hz is often sufficient. Here's the rate tuning pattern: [shows QoS + rate configuration]"
   - *Learning*: Student learns publication rate tradeoffs (bandwidth vs latency)

2. **AI as Student**:
   - *Scenario*: AI suggests standard ROS 2 sensor topic names (/camera/image_raw)
   - *Student teaches*: "Our pipeline uses namespaced topics for multi-robot scenarios. Here's the naming constraint: [specifies namespace pattern]"
   - *Adaptation*: AI configures bridge with namespaced topic structure

3. **AI as Co-Worker**:
   - *Scenario*: Student and AI iterate on clock synchronization
   - *Iteration 1*: Student uses wall clock → ROS 2 bag replay doesn't work
   - *Iteration 2*: AI suggests simulation clock → some nodes don't respect it
   - *Iteration 3*: Together identify using /clock publication with proper QoS → all nodes synchronized
   - *Convergence*: Collaboration solved time synchronization issue

**Content Elements**:
- **ROS 2 bridge architecture**: OmniGraph node-based configuration
- **Manual bridge setup**: Student configures camera + LiDAR publishers
- **AI collaboration**: Tune QoS, optimize performance (show all three roles)
- **Practice exercise**: Students publish sensor data, visualize in RViz, record bag
- **Validation**: Sensor data publishes correctly, RViz visualization works, bag playback successful

**Prerequisites**: Lessons 1-6 (Isaac Sim sensors), Part 1 (ROS 2 topics, QoS)

**Estimated Time**: 120 minutes

---

### Lesson 8: Creating Domain Randomization Skill (Layer 3: Intelligence Design)

**Learning Objective**: Encode domain randomization patterns into reusable skill for future synthetic data generation projects.

**Stage**: 3 (Intelligence Design)

**CEFR Proficiency**: B2

**Reusable Artifact Created**: **isaac-sim-domain-randomization-skill**

**Maps to Evals**: Eval-7.4 (domain randomization patterns)

**Skill Design Framework** (Persona + Questions + Principles):

**Persona Definition**:
"Think like a domain randomization engineer who balances training data diversity with realism. Your goal is to increase model robustness through systematic variation without creating implausible scenes that harm training."

**Question Structure**:
1. What variations increase model robustness for this task?
2. What correlations must be preserved for realism?
3. How much randomization before diminishing returns?
4. What randomization frequency balances diversity vs temporal consistency?
5. How do I validate randomization quality (not too uniform, not too extreme)?

**Principle Articulation**:
1. **Systematic Variation**: Randomize textures, lighting, poses independently but manage correlations (bright rooms have light materials, dim rooms have dark materials)
2. **Plausibility Constraint**: All randomized scenes must be physically plausible (no floating objects, no inverted gravity visualization)
3. **Task-Specific Focus**: Randomize features relevant to task (navigation: floor textures matter; manipulation: object materials matter)
4. **Balance Realism vs Diversity**: Prefer 1000 diverse realistic images over 10,000 unrealistic images

**Content Elements**:
- **Review Lessons 5-6**: What domain randomization patterns did we apply?
- **Pattern extraction**: What's reusable across projects?
- **Skill creation**: Students write skill using Persona + Questions + Principles template
- **Usage validation**: Test skill on new scenario (outdoor scene, different objects)
- **Documentation**: Students document skill invocation pattern for future reuse

**Prerequisites**: Lessons 5-6 (domain randomization experience)

**Estimated Time**: 90 minutes

---

### Lesson 9: Creating Performance Optimization Skill (Layer 3: Intelligence Design)

**Learning Objective**: Encode Isaac Sim performance optimization patterns into reusable skill for real-time simulation and rendering.

**Stage**: 3 (Intelligence Design)

**CEFR Proficiency**: B2

**Reusable Artifact Created**: **isaac-sim-performance-skill**

**Maps to Evals**: Eval-7.1 (GPU acceleration), Eval-7.5 (synthetic data generation performance)

**Skill Design Framework** (Persona + Questions + Principles):

**Persona Definition**:
"Think like a real-time rendering engineer optimizing GPU utilization. Your goal is to maximize simulation speed and rendering quality while staying within hardware constraints (VRAM, GPU compute, memory bandwidth)."

**Question Structure**:
1. What's the performance bottleneck? (Profiling first, don't guess)
2. Which render settings impact quality vs speed? (Samples, bounces, resolution)
3. What LOD (Level of Detail) strategies apply? (Distant objects, off-screen culling)
4. How do I balance batch size for GPU memory? (Larger batches → faster, but risk OOM)
5. When should I use RTX vs rasterization? (Real-time needs vs offline quality)

**Principle Articulation**:
1. **Profile First**: Use nvidia-smi, Isaac Sim profiler to identify bottlenecks before optimizing
2. **LOD for Distant Objects**: High-poly meshes for near camera, low-poly for far objects
3. **Culling Off-Screen Geometry**: Don't render what camera can't see
4. **Batch Size = f(VRAM)**: Tune batch size to GPU memory (8GB → batch 4-8, 24GB → batch 16-32)
5. **Graceful Degradation**: If GPU overloads, drop quality settings before crashing

**Content Elements**:
- **Review Lessons 4-6**: What performance issues did we encounter?
- **Pattern extraction**: What optimization strategies are reusable?
- **Skill creation**: Students write skill using Persona + Questions + Principles template
- **Usage validation**: Test skill on performance-constrained scenario (older GPU, complex scene)
- **Documentation**: Students document profiling workflow and optimization checklist

**Prerequisites**: Lessons 4-7 (Isaac Sim rendering, Replicator)

**Estimated Time**: 90 minutes

---

### Lesson 10: Capstone - Synthetic Training Dataset Generation (Layer 4: Spec-Driven Integration)

**Learning Objective**: Generate production-ready synthetic training dataset (10,000+ images) for object detection using specification-first approach, composing all accumulated skills and knowledge.

**Stage**: 4 (Spec-Driven Integration / Capstone)

**CEFR Proficiency**: B2

**Maps to Evals**: ALL chapter evals integrated

**Project Specification** (Spec FIRST, then implementation):

**Intent**:
Generate synthetic dataset for training humanoid robot object detection model. Dataset must include:
- 10,000+ RGB images with bounding box annotations
- Diverse indoor environments (offices, warehouses, homes)
- Domain randomization for robustness
- Real-time rendering performance (30 FPS minimum)

**Constraints**:
- YOLO annotation format (bounding boxes)
- Objects: household items (chairs, tables, boxes, bottles)
- Camera: Humanoid head-mounted camera (120° FOV, 1280x720)
- Hardware: RTX 4070 GPU, 16GB VRAM

**Success Criteria**:
- [ ] 10,000+ images generated
- [ ] All images have accurate bounding box annotations
- [ ] Visual diversity: 20+ material variations, 10+ lighting conditions, 50+ object poses
- [ ] Performance: Generation rate ≥30 FPS
- [ ] Annotation accuracy: Manual spot-check 100 images, <5% errors

**Component Composition**:
- **Skills from Lessons 8-9**: Apply domain-randomization skill + performance-optimization skill
- **Knowledge from Lessons 3-7**: URDF import, rendering, randomization, Replicator, ROS 2 bridge

**Workflow**:
1. **Specification Writing** (FIRST): Students write detailed spec.md before code
2. **AI Orchestration**: AI implements spec using Replicator API, students validate
3. **Iteration Loop**:
   - Generate first 100 images
   - Validate diversity and annotation accuracy
   - Refine randomization parameters
   - Scale to 10,000 images
4. **Final Validation**: Test on sample object detection training pipeline

**Content Elements**:
- **Spec-first requirement**: Students write specification BEFORE implementation
- **Skill composition**: How do lessons 8-9 skills apply?
- **AI orchestration**: AI generates Replicator script from spec
- **Convergence**: Student validates output, AI refines, iterate until spec satisfied
- **Success validation**: Dataset meets all criteria (quantity, quality, performance)

**Prerequisites**: Lessons 1-9 (all Isaac Sim concepts + skills)

**Estimated Time**: 180 minutes (comprehensive capstone)

---

## IV. Skill Dependencies

**Skill Dependency Graph**:
```
Part 1 URDF skills → Lesson 3 (URDF import to Isaac)
Part 2 Gazebo skills → Lessons 1-2 (compare architectures)

Lesson 3 (URDF import) → Lesson 4 (rendering)
Lesson 4 (rendering) → Lesson 5 (domain randomization)
Lesson 5 (domain randomization) → Lesson 6 (Replicator)
Lesson 6 (Replicator) + Lesson 7 (ROS bridge) → Skills creation
Lesson 8 (domain-randomization skill) → Lesson 10 (capstone)
Lesson 9 (performance skill) → Lesson 10 (capstone)
```

**Cross-Chapter Dependencies**:
- Part 1 (URDF, ROS 2 topics) → Required for Lessons 3, 7
- Part 2 (Gazebo physics) → Conceptual comparison in Lessons 1-2
- Validation: ✅ Prerequisites implemented

---

## V. Assessment Plan

### Formative Assessments (During Lessons)
- Lesson 1: Installation validation checklist (GPU acceleration verified)
- Lesson 3: URDF import success (humanoid stands stable)
- Lesson 5: Domain randomization quality check (100 images reviewed)
- Lesson 7: ROS 2 bridge validation (sensor data in RViz)

### Summative Assessment (End of Chapter)
- Lesson 10: Capstone dataset evaluation (10,000+ images, quality validated, spec satisfied)

**All assessments align with B2 proficiency + Bloom's Apply/Analyze levels**

---

## VI. Validation Checklist

**Chapter-Level Validation**:
- [x] Chapter type identified (technical/code-focused)
- [x] Concept density analysis documented (10 concepts → 9 lessons justified)
- [x] Lesson count justified by concept density + B2 tier
- [x] All evals from spec covered by lessons
- [x] All lessons map to at least one eval

**Stage Progression Validation**:
- [x] Lessons 1-2: Layer 1 (Manual exploration, no AI)
- [x] Lessons 3-7: Layer 2 (AI Collab with Three Roles)
- [x] Lessons 8-9: Layer 3 (Intelligence Design, reusable skills)
- [x] Lesson 10: Layer 4 (Spec-Driven Integration)
- [x] No spec-first before Layer 4

**Cognitive Load Validation**:
- [x] Each lesson's concept count ≤ B2 tier limit (7-10 concepts)
- [x] Complex concepts distributed across lessons (not overloaded)

**Dependency Validation**:
- [x] Skill dependencies satisfied by lesson order
- [x] Cross-chapter dependencies validated (Part 1-2 prerequisites)

**Three Roles Validation** (Layer 2 lessons):
- [x] Lessons 3-7 each demonstrate AI as Teacher
- [x] Lessons 3-7 each demonstrate AI as Student
- [x] Lessons 3-7 each demonstrate AI as Co-Worker (convergence)

**Teaching Modality Validation**:
- [x] Primary: Hands-on discovery (Lessons 1, 4-6)
- [x] Secondary: Socratic dialogue (Lesson 2 - Isaac vs Gazebo tradeoffs)
- [x] Distinct from Part 2 modalities (Part 2: all hands-on; Part 3: adds Socratic)

---

## VII. Anti-Convergence Notes

**Part 2 Patterns Used**: All chapters used hands-on discovery exclusively

**Part 3 Chapter 7 Variation**:
- **Primary**: Hands-on discovery (Lessons 1, 4-6 explore Isaac Sim interface)
- **Secondary**: Socratic dialogue (Lesson 2 forces reasoning about Isaac vs Gazebo tradeoffs)
- **Rationale**: Isaac Sim's rich UI benefits from exploration. Socratic questions activate critical thinking about tool selection (when Isaac? when Gazebo? why?).

**Result**: Teaching modality varies from Part 2 (adds Socratic), sets up distinct modalities for Chapters 8-9.

---

**End of Chapter 7 Lesson Plan**
